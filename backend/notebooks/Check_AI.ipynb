{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "951bba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang chạy local nên bỏ qua bước copy từ /kaggle/input.\n",
      "✔️ Đang sử dụng detector local tại D:\\Đồ án tốt nghiệp\\CheckAI\\detector_phobert\n",
      "✔️ Đã load detector từ detector_phobert\n",
      "{\n",
      "  \"model\": {\n",
      "    \"prob_ai\": 0.5000000008172898,\n",
      "    \"label\": \"AI\",\n",
      "    \"threshold\": 0.5,\n",
      "    \"n_windows\": 2,\n",
      "    \"prob_ai_mean\": 0.5000000008172898,\n",
      "    \"prob_ai_max\": 1.0\n",
      "  },\n",
      "  \"combined_prob_ai\": 0.8848662199924868,\n",
      "  \"combined_label\": \"AI\",\n",
      "  \"combined_threshold\": 0.5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==== LOAD MODEL TỪ DATASET & CHẠY SUY LUẬN ====\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import shutil\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM\n",
    "import unicodedata\n",
    "\n",
    "DEFAULT_MODEL_NAME = 'vinai/phobert-base'\n",
    "DEFAULT_MAX_SEQ_LEN = 256\n",
    "DETECTOR_AGG = 'mean'  # mean|max: mean = AI phải chiếm tỷ lệ đáng kể trong toàn bài\n",
    "CHUNK_STRIDE_TOKENS = 128  # khớp Train.py (CHUNK_STRIDE)\n",
    "DETECTOR_MAX_WINDOWS = None  # khớp Train.py: không cap số cửa sổ (lấy hết chunks)\n",
    "COMBINED_THRESHOLD = 0.5  # nhãn cho combined_prob_ai\n",
    "\n",
    "if 'IS_KAGGLE' not in globals():\n",
    "    IS_KAGGLE = pathlib.Path('/kaggle/input').exists()\n",
    "if 'ROOT' not in globals():\n",
    "    ROOT = pathlib.Path('/kaggle/working') if IS_KAGGLE else pathlib.Path.cwd()\n",
    "if 'ARTIFACT_DIR' not in globals():\n",
    "    ARTIFACT_DIR = ROOT / 'artifacts'\n",
    "if 'DEVICE' not in globals():\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if 'MODEL_NAME' not in globals():\n",
    "    MODEL_NAME = DEFAULT_MODEL_NAME\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if 'MAX_SEQ_LEN' not in globals():\n",
    "    MAX_SEQ_LEN = min(DEFAULT_MAX_SEQ_LEN, getattr(tokenizer, 'model_max_length', DEFAULT_MAX_SEQ_LEN) or DEFAULT_MAX_SEQ_LEN)\n",
    "\n",
    "import regex as re\n",
    "SENT_SPLIT = re.compile(r'(?<=[.!?…])\\s+')\n",
    "WORD_RE = re.compile(r\"\\p{L}+(?:['’]\\p{L}+)?\", re.UNICODE)\n",
    "\n",
    "\n",
    "# ---- Text normalization (giúp ổn định output; tránh unicode/space làm flip) ----\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    cleaned = unicodedata.normalize('NFC', text or '')\n",
    "    transl_table = str.maketrans({'“': '\"', '”': '\"', '’': \"'\", '–': '-', '—': '-'})\n",
    "    cleaned = cleaned.translate(transl_table)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "# ---- Optional: MaskedLM pseudo-perplexity model (nhỏ, dùng làm feature) ----\n",
    "\n",
    "import math\n",
    "import zlib\n",
    "from collections import Counter\n",
    "\n",
    "PPL_MODEL_NAME = 'vinai/phobert-base'\n",
    "PPL_MAX_TOKENS = 96        # cap tokens để không quá chậm\n",
    "PPL_STEP = 6              # mask mỗi k token (lấy mẫu)\n",
    "PPL_MAX_POSITIONS = 24    # cap số vị trí mask\n",
    "\n",
    "# Token-level advanced features\n",
    "TOKEN_FULL_MAX_CHARS = 8000  # tránh tokenize full text quá dài (chậm)\n",
    "\n",
    "if 'mlm_model' not in globals():\n",
    "    mlm_model = None\n",
    "\n",
    "\n",
    "def load_mlm_model(model_name: str = PPL_MODEL_NAME):\n",
    "    global mlm_model\n",
    "    if mlm_model is not None:\n",
    "        return mlm_model\n",
    "    try:\n",
    "        m = AutoModelForMaskedLM.from_pretrained(model_name).to(DEVICE)\n",
    "        m.eval()\n",
    "        mlm_model = m\n",
    "        print('✔️ Đã load MaskedLM cho pseudo-perplexity:', model_name)\n",
    "        return mlm_model\n",
    "    except Exception as e:\n",
    "        print('⚠️ Không load được MaskedLM (pseudo-perplexity). Lý do:', repr(e))\n",
    "        print('   Gợi ý: kiểm tra internet/cache HF hoặc đổi PPL_MODEL_NAME.')\n",
    "        mlm_model = None\n",
    "        return None\n",
    "\n",
    "\n",
    "def maskedlm_pseudo_ppl(text: str) -> float:\n",
    "    # Pseudo-perplexity cho MaskedLM: mask token (lấy mẫu) và tính -log P(token|context).\n",
    "    if mlm_model is None:\n",
    "        return 0.0\n",
    "    if tokenizer.mask_token_id is None:\n",
    "        return 0.0\n",
    "    clean = normalize_text(text)\n",
    "    if not clean:\n",
    "        return 0.0\n",
    "\n",
    "    enc = tokenizer(clean, return_tensors='pt', truncation=True, max_length=PPL_MAX_TOKENS)\n",
    "    input_ids = enc['input_ids'][0].to(DEVICE)\n",
    "    attn = enc.get('attention_mask', None)\n",
    "    if attn is None:\n",
    "        attn = torch.ones_like(input_ids)\n",
    "    else:\n",
    "        attn = attn[0].to(DEVICE)\n",
    "\n",
    "    special = set(getattr(tokenizer, 'all_special_ids', []) or [])\n",
    "    valid_positions = []\n",
    "    for i, (tid, m) in enumerate(zip(input_ids.tolist(), attn.tolist())):\n",
    "        if m != 1:\n",
    "            continue\n",
    "        if tid in special:\n",
    "            continue\n",
    "        valid_positions.append(i)\n",
    "    if not valid_positions:\n",
    "        return 0.0\n",
    "\n",
    "    positions = valid_positions[::max(1, int(PPL_STEP))]\n",
    "    positions = positions[:int(PPL_MAX_POSITIONS)]\n",
    "    if not positions:\n",
    "        return 0.0\n",
    "\n",
    "    total_nll = 0.0\n",
    "    n = 0\n",
    "    with torch.inference_mode():\n",
    "        for pos in positions:\n",
    "            masked_ids = input_ids.clone()\n",
    "            true_id = masked_ids[pos].item()\n",
    "            masked_ids[pos] = tokenizer.mask_token_id\n",
    "            out = mlm_model(input_ids=masked_ids.unsqueeze(0), attention_mask=attn.unsqueeze(0))\n",
    "            logits = out.logits[0, pos]\n",
    "            logp = torch.log_softmax(logits, dim=-1)[true_id].item()\n",
    "            total_nll += -logp\n",
    "            n += 1\n",
    "    nll = total_nll / max(n, 1)\n",
    "    ppl = math.exp(min(nll, 20.0))\n",
    "    return float(ppl)\n",
    "\n",
    "\n",
    "# ---- Model prediction ----\n",
    "\n",
    "\n",
    "def _predict_prob_from_ids(input_ids_1d: list, model) -> float:\n",
    "    ids = torch.tensor([input_ids_1d], device=DEVICE)\n",
    "    attn = torch.ones_like(ids)\n",
    "    with torch.inference_mode():\n",
    "        logits = model(input_ids=ids, attention_mask=attn).logits\n",
    "        prob_ai = torch.softmax(logits, dim=-1)[0, 1].item()\n",
    "    return float(prob_ai)\n",
    "\n",
    "\n",
    "def detector_predict(\n",
    "    text: str,\n",
    "    model,\n",
    "    threshold: float = 0.5,\n",
    "    agg: str = 'mean',\n",
    "    stride_tokens: Optional[int] = None,\n",
    "    stride_ratio: float = 0.5,\n",
    "    max_windows: Optional[int] = None,\n",
    " ) -> Dict[str, Any]:\n",
    "    \"\"\"Predict trên *toàn văn* bằng cách chạy nhiều cửa sổ token (nếu text dài).\n",
    "\n",
    "    - `agg='mean'` ổn định hơn (phương án B: AI phải chiếm tỷ lệ đáng kể).\n",
    "    - `agg='max'` nhạy với 1 đoạn AI mạnh.\n",
    "    - Ưu tiên `stride_tokens` để đồng bộ với Train.py; nếu None thì dùng `stride_ratio`.\n",
    "    - `max_windows=None` để khớp Train.py (không cap số chunk/cửa sổ).\n",
    "\n",
    "    Trả về đúng 1 `prob_ai` (không trả highlight).\n",
    "    \"\"\"\n",
    "    clean_text = normalize_text(text)\n",
    "    if not clean_text:\n",
    "        return {'prob_ai': 0.0, 'label': 'Human', 'threshold': threshold}\n",
    "\n",
    "    try:\n",
    "        ids_no_special = (tokenizer(clean_text, add_special_tokens=False, truncation=False).get('input_ids', []) or [])\n",
    "    except Exception:\n",
    "        # fallback: nếu tokenize full lỗi thì dùng truncate như cũ\n",
    "        inputs = tokenizer(clean_text, return_tensors='pt', truncation=True, max_length=MAX_SEQ_LEN).to(DEVICE)\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**inputs).logits\n",
    "            prob_ai = torch.softmax(logits, dim=-1)[0, 1].item()\n",
    "        prob_ai_out = float(prob_ai)\n",
    "        label = 'AI' if prob_ai_out >= threshold else 'Human'\n",
    "        return {'prob_ai': prob_ai_out, 'label': label, 'threshold': threshold}\n",
    "\n",
    "    n_special = int(getattr(tokenizer, 'num_special_tokens_to_add', lambda pair=False: 2)(pair=False) or 2)\n",
    "    win_len = max(8, int(MAX_SEQ_LEN) - n_special)\n",
    "\n",
    "    probs = None\n",
    "    if len(ids_no_special) <= win_len:\n",
    "        ids = tokenizer.build_inputs_with_special_tokens(ids_no_special)\n",
    "        prob_ai = _predict_prob_from_ids(ids, model)\n",
    "    else:\n",
    "        if stride_tokens is not None:\n",
    "            stride = max(1, int(stride_tokens))\n",
    "        else:\n",
    "            stride = max(1, int(win_len * float(stride_ratio)))\n",
    "        stride = min(stride, win_len)\n",
    "\n",
    "        probs = []\n",
    "        for start in range(0, len(ids_no_special), stride):\n",
    "            chunk = ids_no_special[start:start + win_len]\n",
    "            if len(chunk) < 8:\n",
    "                break\n",
    "            ids = tokenizer.build_inputs_with_special_tokens(chunk)\n",
    "            probs.append(_predict_prob_from_ids(ids, model))\n",
    "            if start + win_len >= len(ids_no_special):\n",
    "                break\n",
    "            if max_windows is not None and len(probs) >= int(max_windows):\n",
    "                break\n",
    "\n",
    "        if not probs:\n",
    "            ids = tokenizer(clean_text, add_special_tokens=True, truncation=True, max_length=MAX_SEQ_LEN).get('input_ids', [])\n",
    "            prob_ai = _predict_prob_from_ids(ids, model)\n",
    "        else:\n",
    "            mean_v = float(np.mean(probs))\n",
    "            max_v = float(np.max(probs))\n",
    "            if agg == 'mean':\n",
    "                prob_ai = mean_v\n",
    "            else:\n",
    "                prob_ai = max_v\n",
    "\n",
    "    prob_ai_out = float(prob_ai)\n",
    "    label = 'AI' if prob_ai_out >= threshold else 'Human'\n",
    "    out = {'prob_ai': prob_ai_out, 'label': label, 'threshold': threshold}\n",
    "    if probs:\n",
    "        out['n_windows'] = int(len(probs))\n",
    "        out['prob_ai_mean'] = float(np.mean(probs))\n",
    "        out['prob_ai_max'] = float(np.max(probs))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- Extra features (thuộc tính văn bản) ----\n",
    "\n",
    "# Regex helpers\n",
    "PUNCT_RE = re.compile(r\"[\\p{P}\\p{S}]\", re.UNICODE)\n",
    "DIGIT_RE = re.compile(r\"\\p{N}\", re.UNICODE)\n",
    "LETTER_RE = re.compile(r\"\\p{L}\", re.UNICODE)\n",
    "UPPER_RE = re.compile(r\"\\p{Lu}\", re.UNICODE)\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\", re.IGNORECASE)\n",
    "QUOTE_RE = re.compile(r\"['\\\"“”‘’]\", re.UNICODE)\n",
    "BULLET_LINE_RE = re.compile(r\"(?m)^\\s*(?:[-*•]|\\d+[\\).]|[a-zA-Z][\\).])\\s+\", re.UNICODE)\n",
    "\n",
    "# Stopwords (nhỏ/gọn)\n",
    "VI_STOPWORDS = {\n",
    "    'và','là','của','cho','một','những','các','đã','đang','sẽ','với','trong','khi','để','này','đó','ở',\n",
    "    'tôi','bạn','anh','chị','em','chúng','chúng tôi','chúng ta','họ','nó','thì','như','vì','nên',\n",
    "    'cũng','rất','không','có','đến','từ','ra','vào','lại','hay','được','bị','vẫn','nhiều','ít'\n",
    "}\n",
    "\n",
    "\n",
    "def _words(s: str):\n",
    "    return WORD_RE.findall(s.lower() if isinstance(s, str) else '')\n",
    "\n",
    "\n",
    "def compute_text_features(text: str) -> Dict[str, float]:\n",
    "    raw = text or ''\n",
    "    t = normalize_text(raw)\n",
    "    chars = len(t)\n",
    "    raw_chars = len(raw)\n",
    "\n",
    "    punct = len(PUNCT_RE.findall(t))\n",
    "    digits = len(DIGIT_RE.findall(t))\n",
    "\n",
    "    url_count = len(URL_RE.findall(raw))\n",
    "    email_count = len(EMAIL_RE.findall(raw))\n",
    "    quote_count = len(QUOTE_RE.findall(raw))\n",
    "    bullet_lines = len(BULLET_LINE_RE.findall(raw))\n",
    "    newline_count = raw.count('\\n')\n",
    "\n",
    "    letters = len(LETTER_RE.findall(t))\n",
    "    uppers = len(UPPER_RE.findall(t))\n",
    "    uppercase_ratio = (uppers / (letters + 1e-6)) if letters else 0.0\n",
    "\n",
    "    words = _words(t)\n",
    "    n_words = len(words)\n",
    "    uniq = len(set(words)) if n_words else 0\n",
    "    uniq_ratio = (uniq / n_words) if n_words else 0.0\n",
    "    avg_word_len = (sum(len(w) for w in words) / n_words) if n_words else 0.0\n",
    "\n",
    "    sents = [s.strip() for s in SENT_SPLIT.split(t) if s.strip()]\n",
    "    sent_lens = [len(_words(s)) for s in sents] if sents else []\n",
    "    n_sents = len(sent_lens)\n",
    "    avg_sent_len = (sum(sent_lens) / n_sents) if n_sents else 0.0\n",
    "    var_sent = (sum((x - avg_sent_len) ** 2 for x in sent_lens) / n_sents) if n_sents else 0.0\n",
    "    std_sent = math.sqrt(var_sent) if n_sents else 0.0\n",
    "    burstiness = (std_sent / (avg_sent_len + 1e-6)) if n_sents else 0.0\n",
    "\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n+\", raw) if p.strip()]\n",
    "    n_paras = len(paras) if paras else 0\n",
    "    para_lens = [len(_words(normalize_text(p))) for p in paras] if paras else []\n",
    "    avg_para_len = (sum(para_lens) / n_paras) if n_paras else 0.0\n",
    "\n",
    "    max_rep = 0.0\n",
    "    hapax_ratio = 0.0\n",
    "    entropy = 0.0\n",
    "    herdan_c = 0.0\n",
    "    stopword_ratio = 0.0\n",
    "    if n_words:\n",
    "        c = Counter(words)\n",
    "        max_rep = max(c.values()) / n_words\n",
    "        hapax_ratio = sum(1 for _, v in c.items() if v == 1) / n_words\n",
    "        probs = [v / n_words for v in c.values()]\n",
    "        ent = -sum(p * math.log(p + 1e-12) for p in probs)\n",
    "        entropy = ent / (math.log(len(c) + 1e-12) + 1e-6)\n",
    "        herdan_c = (math.log(len(c) + 1e-12) / (math.log(n_words + 1e-12) + 1e-6)) if n_words > 1 else 0.0\n",
    "        stopword_ratio = sum(1 for w in words if w in VI_STOPWORDS) / n_words\n",
    "\n",
    "    bigram_rep = 0.0\n",
    "    trigram_rep = 0.0\n",
    "    if n_words >= 2:\n",
    "        bigrams = list(zip(words, words[1:]))\n",
    "        bigram_rep = 1.0 - (len(set(bigrams)) / (len(bigrams) + 1e-6))\n",
    "    if n_words >= 3:\n",
    "        trigrams = list(zip(words, words[1:], words[2:]))\n",
    "        trigram_rep = 1.0 - (len(set(trigrams)) / (len(trigrams) + 1e-6))\n",
    "\n",
    "    compression_ratio = 0.0\n",
    "    if raw_chars > 0:\n",
    "        try:\n",
    "            comp = zlib.compress(raw.encode('utf-8', errors='ignore'), level=9)\n",
    "            compression_ratio = len(comp) / (raw_chars + 1e-6)\n",
    "        except Exception:\n",
    "            compression_ratio = 0.0\n",
    "\n",
    "    ppl = 0.0\n",
    "    if mlm_model is not None:\n",
    "        try:\n",
    "            ppl = maskedlm_pseudo_ppl(raw)\n",
    "        except Exception:\n",
    "            ppl = 0.0\n",
    "\n",
    "    # Advanced tokenizer-level features (đúng theo cách model nhìn text)\n",
    "    hf_tokens_trunc = 0\n",
    "    hf_tokens_full = -1\n",
    "    hf_is_truncated = 0.0\n",
    "    hf_subwords_per_word = 0.0\n",
    "    try:\n",
    "        enc_trunc = tokenizer(t, add_special_tokens=True, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        hf_tokens_trunc = len(enc_trunc.get('input_ids', []) or [])\n",
    "        hf_subwords_per_word = float(hf_tokens_trunc / (n_words + 1e-6)) if n_words else 0.0\n",
    "        if chars <= TOKEN_FULL_MAX_CHARS:\n",
    "            enc_full = tokenizer(t, add_special_tokens=True, truncation=False)\n",
    "            hf_tokens_full = len(enc_full.get('input_ids', []) or [])\n",
    "            hf_is_truncated = 1.0 if hf_tokens_full > MAX_SEQ_LEN else 0.0\n",
    "        else:\n",
    "            hf_tokens_full = -1\n",
    "            hf_is_truncated = 1.0 if hf_tokens_trunc >= MAX_SEQ_LEN else 0.0\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'n_chars': float(chars),\n",
    "        'n_words': float(n_words),\n",
    "        'n_sents': float(n_sents),\n",
    "        'n_paras': float(n_paras),\n",
    "\n",
    "        'punct_ratio': float(punct / (chars + 1e-6)),\n",
    "        'digit_ratio': float(digits / (chars + 1e-6)),\n",
    "        'uppercase_ratio': float(uppercase_ratio),\n",
    "        'quote_ratio': float(quote_count / (raw_chars + 1e-6)),\n",
    "        'newline_ratio': float(newline_count / (raw_chars + 1e-6)),\n",
    "\n",
    "        'unique_word_ratio': float(uniq_ratio),\n",
    "        'avg_word_len': float(avg_word_len),\n",
    "        'hapax_ratio': float(hapax_ratio),\n",
    "        'herdan_c': float(herdan_c),\n",
    "        'entropy_norm': float(entropy),\n",
    "        'stopword_ratio': float(stopword_ratio),\n",
    "\n",
    "        'avg_sent_len_words': float(avg_sent_len),\n",
    "        'avg_para_len_words': float(avg_para_len),\n",
    "        'burstiness': float(burstiness),\n",
    "        'bullet_lines': float(bullet_lines),\n",
    "        'url_count': float(url_count),\n",
    "        'email_count': float(email_count),\n",
    "\n",
    "        'max_unigram_rep': float(max_rep),\n",
    "        'bigram_rep': float(bigram_rep),\n",
    "        'trigram_rep': float(trigram_rep),\n",
    "        'compression_ratio': float(compression_ratio),\n",
    "\n",
    "        'mlm_pseudo_ppl': float(ppl),\n",
    "\n",
    "        # tokenizer-level\n",
    "        'hf_tokens_trunc': float(hf_tokens_trunc),\n",
    "        'hf_tokens_full': float(hf_tokens_full),\n",
    "        'hf_is_truncated': float(hf_is_truncated),\n",
    "        'hf_subwords_per_word': float(hf_subwords_per_word),\n",
    "    }\n",
    "\n",
    "\n",
    "def feature_ai_score_breakdown(feat: Dict[str, float]) -> Dict[str, float]:\n",
    "    # Breakdown để bạn kiểm tra từng điểm con + trọng số.\n",
    "    b = feat.get('burstiness', 0.0)\n",
    "    u = feat.get('unique_word_ratio', 0.0)\n",
    "    ent = feat.get('entropy_norm', 0.0)\n",
    "    r2 = feat.get('bigram_rep', 0.0)\n",
    "    r3 = feat.get('trigram_rep', 0.0)\n",
    "    comp = feat.get('compression_ratio', 0.0)\n",
    "    p = feat.get('punct_ratio', 0.0)\n",
    "    bullets = feat.get('bullet_lines', 0.0)\n",
    "    ppl = feat.get('mlm_pseudo_ppl', 0.0)\n",
    "    spw = feat.get('hf_subwords_per_word', 0.0)\n",
    "    is_trunc = feat.get('hf_is_truncated', 0.0)\n",
    "\n",
    "    score_b = 1.0 / (1.0 + math.exp(4.0 * (b - 0.6)))\n",
    "    score_r = min(max((0.6 * r2 + 0.4 * r3) / 0.30, 0.0), 1.0)\n",
    "    score_ent = min(max((0.55 - ent) / 0.35, 0.0), 1.0)\n",
    "    score_u = min(max((0.55 - u) / 0.35, 0.0), 1.0)\n",
    "    score_c = min(max((1.05 - comp) / 0.45, 0.0), 1.0)\n",
    "    score_fmt = 1.0 - min(max((p - 0.12) / 0.25, 0.0), 1.0)\n",
    "    score_bul = 1.0 - min(max(bullets / 8.0, 0.0), 1.0)\n",
    "\n",
    "    score_ppl = 0.0\n",
    "    if ppl and ppl > 0:\n",
    "        score_ppl = min(max((60.0 - ppl) / 50.0, 0.0), 1.0)\n",
    "\n",
    "    score_spw = min(max((spw - 1.4) / 0.9, 0.0), 1.0)\n",
    "    score_trunc = float(min(max(is_trunc, 0.0), 1.0))\n",
    "\n",
    "    # Trọng số mới: tăng nhạy với (burstiness thấp + pseudo-ppl thấp) — kiểu ‘AI viết trơn tru’.\n",
    "    w_b = 0.32\n",
    "    w_ppl = 0.20\n",
    "    w_c = 0.12\n",
    "    w_fmt = 0.08\n",
    "    w_bul = 0.03\n",
    "    w_r = 0.10\n",
    "    w_ent = 0.05\n",
    "    w_u = 0.03\n",
    "    w_spw = 0.05\n",
    "    w_trunc = 0.02\n",
    "\n",
    "    raw = (\n",
    "        w_b * score_b +\n",
    "        w_r * score_r +\n",
    "        w_ent * score_ent +\n",
    "        w_u * score_u +\n",
    "        w_c * score_c +\n",
    "        w_fmt * score_fmt +\n",
    "        w_bul * score_bul +\n",
    "        w_ppl * score_ppl +\n",
    "        w_spw * score_spw +\n",
    "        w_trunc * score_trunc\n",
    "    )\n",
    "    raw = float(min(max(raw, 0.0), 1.0))\n",
    "\n",
    "    return {\n",
    "        'score_b': float(score_b),\n",
    "        'score_r': float(score_r),\n",
    "        'score_ent': float(score_ent),\n",
    "        'score_u': float(score_u),\n",
    "        'score_c': float(score_c),\n",
    "        'score_fmt': float(score_fmt),\n",
    "        'score_bul': float(score_bul),\n",
    "        'score_ppl': float(score_ppl),\n",
    "        'score_spw': float(score_spw),\n",
    "        'score_trunc': float(score_trunc),\n",
    "        'w_b': w_b, 'w_r': w_r, 'w_ent': w_ent, 'w_u': w_u, 'w_c': w_c,\n",
    "        'w_fmt': w_fmt, 'w_bul': w_bul, 'w_ppl': w_ppl, 'w_spw': w_spw, 'w_trunc': w_trunc,\n",
    "        'feature_score_ai': raw,\n",
    "    }\n",
    "\n",
    "\n",
    "def feature_ai_score(feat: Dict[str, float]) -> float:\n",
    "    # Lưu ý: heuristic chỉ là phụ trợ; nếu muốn ‘phản ánh tốt’ nhất nên học trọng số từ Ai_human.csv.\n",
    "    return float(feature_ai_score_breakdown(feat).get('feature_score_ai', 0.0))\n",
    "\n",
    "\n",
    "def combine_scores(prob_ai: float, feat_score: float, w_model: float = 0.6) -> float:\n",
    "    w = float(min(max(w_model, 0.0), 1.0))\n",
    "    return float(w * prob_ai + (1.0 - w) * feat_score)\n",
    "\n",
    "\n",
    "# ---- Final decision (model + features) ----\n",
    "\n",
    "def ensemble_decision(text: str, clf_model=None, threshold: float = 0.5, w_model: float = 0.6) -> Dict[str, Any]:\n",
    "    if clf_model is None:\n",
    "        return {\n",
    "            'model': None,\n",
    "            'combined_prob_ai': None,\n",
    "            'combined_label': None,\n",
    "            'combined_threshold': float(COMBINED_THRESHOLD),\n",
    "        }\n",
    "\n",
    "    # Predict toàn văn (không còn bị “cắt 256 token” như trước)\n",
    "    clf_entry = detector_predict(text, clf_model, threshold=threshold, agg=DETECTOR_AGG, stride_tokens=CHUNK_STRIDE_TOKENS, max_windows=DETECTOR_MAX_WINDOWS)\n",
    "    feat = compute_text_features(text)\n",
    "    feat_score = feature_ai_score(feat)\n",
    "\n",
    "    prob_model = float(clf_entry['prob_ai'])\n",
    "    combined = combine_scores(prob_model, feat_score, w_model=w_model)\n",
    "\n",
    "    return {\n",
    "        'model': {**clf_entry},\n",
    "        'combined_prob_ai': float(combined),\n",
    "        'combined_label': 'AI' if float(combined) >= float(COMBINED_THRESHOLD) else 'Human',\n",
    "        'combined_threshold': float(COMBINED_THRESHOLD),\n",
    "    }\n",
    "\n",
    "\n",
    "COMBINER_DEBIAS_LENGTH = True  # giảm bias theo độ dài của combiner\n",
    "COMBINER_LENGTH_FEATURES = ['n_chars','n_words','n_sents','n_paras','avg_para_len_words','hf_tokens_full','hf_tokens_trunc','hf_is_truncated','hf_subwords_per_word']\n",
    "\n",
    "def _combiner_prob_ai(combiner_model, X_row: pd.DataFrame) -> float:\n",
    "\n",
    "\n",
    "    proba = combiner_model.predict_proba(X_row)[0]\n",
    "\n",
    "    classes = getattr(combiner_model, 'classes_', None)\n",
    "    last_est = None\n",
    "    if hasattr(combiner_model, 'steps') and getattr(combiner_model, 'steps', None):\n",
    "        last_est = combiner_model.steps[-1][1]\n",
    "        if classes is None:\n",
    "            classes = getattr(last_est, 'classes_', None)\n",
    "\n",
    "    ai_idx = None\n",
    "\n",
    "    # 1) Label chuỗi\n",
    "    if classes is not None:\n",
    "        try:\n",
    "            cls_list = list(classes)\n",
    "            for i, c in enumerate(cls_list):\n",
    "                if isinstance(c, str) and c.strip().lower() == 'ai':\n",
    "                    ai_idx = i\n",
    "                    break\n",
    "            if ai_idx is None:\n",
    "                for i, c in enumerate(cls_list):\n",
    "                    if isinstance(c, str) and 'ai' in c.lower():\n",
    "                        ai_idx = i\n",
    "                        break\n",
    "        except Exception:\n",
    "            ai_idx = None\n",
    "\n",
    "    # 2) Nhãn số: suy ra bằng dấu coef(prob_model)\n",
    "    if ai_idx is None:\n",
    "        try:\n",
    "            if 'prob_model' in X_row.columns:\n",
    "                coef_source = last_est if last_est is not None else combiner_model\n",
    "                if hasattr(coef_source, 'coef_'):\n",
    "                    pm_idx = list(X_row.columns).index('prob_model')\n",
    "                    pm_coef = float(coef_source.coef_[0, pm_idx])\n",
    "                    ai_idx = 1 if pm_coef >= 0 else 0\n",
    "        except Exception:\n",
    "            ai_idx = None\n",
    "\n",
    "    # 3) Fallback: nếu classes có số 1 thì coi đó là AI\n",
    "    if ai_idx is None and classes is not None:\n",
    "        try:\n",
    "            cls_list = list(classes)\n",
    "            if 1 in cls_list:\n",
    "                ai_idx = cls_list.index(1)\n",
    "        except Exception:\n",
    "            ai_idx = None\n",
    "\n",
    "    if ai_idx is None:\n",
    "        ai_idx = 1 if len(proba) > 1 else 0\n",
    "\n",
    "    return float(proba[ai_idx])\n",
    "\n",
    "def combiner_top_contrib(combiner_model, X_row: pd.DataFrame, ai_idx: int = 1, top_k: int = 12) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        if not hasattr(combiner_model, 'steps'):\n",
    "            return None\n",
    "        steps = getattr(combiner_model, 'steps', []) or []\n",
    "        scaler = None\n",
    "        for _, est in steps:\n",
    "            if hasattr(est, 'mean_') and hasattr(est, 'scale_'):\n",
    "                scaler = est\n",
    "                break\n",
    "        if not steps:\n",
    "            return None\n",
    "        clf = steps[-1][1]\n",
    "        if not hasattr(clf, 'coef_'):\n",
    "            return None\n",
    "        cols = list(X_row.columns)\n",
    "        x = X_row.iloc[0].astype(float).values\n",
    "        if scaler is not None:\n",
    "            mean = np.asarray(scaler.mean_, dtype=float)\n",
    "            scale = np.asarray(scaler.scale_, dtype=float)\n",
    "            scale = np.where(scale == 0, 1.0, scale)\n",
    "            z = (x - mean) / scale\n",
    "        else:\n",
    "            z = x\n",
    "        coef = np.asarray(clf.coef_, dtype=float)\n",
    "        if coef.ndim == 2:\n",
    "            coef = coef[0]\n",
    "        intercept = float(np.asarray(getattr(clf, 'intercept_', [0.0]), dtype=float).ravel()[0])\n",
    "        contrib = coef * z\n",
    "        logit = float(contrib.sum() + intercept)\n",
    "        if int(ai_idx) == 0:\n",
    "            contrib = -contrib\n",
    "            logit = -logit\n",
    "            intercept = -intercept\n",
    "        out_coef = coef if int(ai_idx) == 1 else -coef\n",
    "        df = pd.DataFrame({'feature': cols, 'raw': x, 'scaled': z, 'coef': out_coef, 'contrib': contrib})\n",
    "        df['abs_contrib'] = df['contrib'].abs()\n",
    "        df = df.sort_values('abs_contrib', ascending=False)\n",
    "        head = df.head(int(top_k)).copy()\n",
    "        summary = pd.DataFrame({\n",
    "            'feature': ['__INTERCEPT__', '__LOGIT__'],\n",
    "            'raw': [np.nan, np.nan],\n",
    "            'scaled': [np.nan, np.nan],\n",
    "            'coef': [np.nan, np.nan],\n",
    "            'contrib': [intercept, logit],\n",
    "            'abs_contrib': [abs(intercept), abs(logit)],\n",
    "        })\n",
    "        return pd.concat([summary, head], ignore_index=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "SAMPLE_TEXT = \"\"\"Tháng trước, chính quyền Tổng thống Donald Trump đã đề xuất một khuôn khổ hòa bình nhằm hướng tới việc chấm dứt xung đột tại Ukraine. Kế hoạch này sau đó được điều chỉnh nhiều lần, bao gồm các nội dung như Ukraine từ bỏ mục tiêu gia nhập NATO và chấp nhận nhượng một số vùng lãnh thổ cho Nga. Đổi lại, Kiev sẽ nhận được các bảo đảm an ninh, tuy nhiên các cam kết này hiện chưa được mô tả chi tiết.\n",
    "\n",
    "Phát biểu tại Nhà Trắng hôm thứ Hai, khi được hỏi về lý do Ukraine có thể chấp nhận việc mất lãnh thổ, ông Trump cho rằng đây là thực trạng đã xảy ra. Theo ông, những khu vực đó trên thực tế không còn nằm dưới sự kiểm soát của Ukraine. Ông cũng cho biết Mỹ đang tiếp tục làm việc để xây dựng các cơ chế bảo đảm an ninh nhằm hạn chế nguy cơ xung đột tái diễn.\n",
    "\n",
    "Tổng thống Mỹ cho biết ông đã có các cuộc trao đổi trực tiếp với Tổng thống Nga Vladimir Putin và đánh giá rằng Nga đang thể hiện mong muốn chấm dứt chiến sự. Tuy nhiên, ông nhận định lập trường của cả Nga và Ukraine vẫn có sự thay đổi theo thời điểm, khiến tiến trình đàm phán gặp nhiều khó khăn. Mục tiêu của Mỹ, theo ông Trump, là đưa các bên liên quan về một lập trường chung.\n",
    "\n",
    "Sau cuộc gặp tại Berlin giữa đặc phái viên Steve Witkoff, ông Jared Kushner và phái đoàn Ukraine, Tổng thống Trump đánh giá các cuộc thảo luận diễn ra theo hướng tích cực, kéo dài và mang tính xây dựng, đồng thời cho rằng tiến trình đàm phán đang có những bước tiến nhất định.\"\"\"\n",
    "\n",
    "MODEL_DATASET_SLUG = 'model-check-ai'  # đổi theo tên dataset bạn upload\n",
    "MODEL_RELATIVE_PATH = pathlib.Path('detector_phobert')\n",
    "LOCAL_MODEL_PATH = pathlib.Path('detector_phobert')\n",
    "eval_path = None\n",
    "if IS_KAGGLE:\n",
    "    remote_dir = pathlib.Path('/kaggle/input') / MODEL_DATASET_SLUG / MODEL_RELATIVE_PATH\n",
    "    if remote_dir.exists():\n",
    "        target_dir = ARTIFACT_DIR / 'detector_phobert'\n",
    "        target_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copytree(remote_dir, target_dir, dirs_exist_ok=True)\n",
    "        eval_path = target_dir\n",
    "        print('✔️ Đã copy detector từ', remote_dir)\n",
    "    else:\n",
    "        eval_path = ARTIFACT_DIR / 'detector_phobert'\n",
    "        print('⚠️ Không tìm thấy', remote_dir, '- kiểm tra MODEL_DATASET_SLUG hoặc cấu trúc dataset.')\n",
    "else:\n",
    "    print('Đang chạy local nên bỏ qua bước copy từ /kaggle/input.')\n",
    "    if LOCAL_MODEL_PATH.exists():\n",
    "        eval_path = LOCAL_MODEL_PATH\n",
    "        print('✔️ Đang sử dụng detector local tại', LOCAL_MODEL_PATH.resolve())\n",
    "    else:\n",
    "        eval_path = ARTIFACT_DIR / 'detector_phobert'\n",
    "        print('⚠️ Không thấy LOCAL_MODEL_PATH tại', LOCAL_MODEL_PATH.resolve())\n",
    "        print('   -> Thử load từ thư mục artifacts:', eval_path)\n",
    "\n",
    "if 'classifier_model' in globals():\n",
    "    del classifier_model\n",
    "classifier_model = None\n",
    "\n",
    "if eval_path and eval_path.exists():\n",
    "    classifier_model = AutoModelForSequenceClassification.from_pretrained(str(eval_path)).to(DEVICE)\n",
    "    classifier_model.eval()\n",
    "    print('✔️ Đã load detector từ', eval_path)\n",
    "\n",
    "    # Tải MaskedLM để tính pseudo-perplexity (feature)\n",
    "    _ = load_mlm_model(PPL_MODEL_NAME)\n",
    "\n",
    "    # ---- Output: ưu tiên 1 kết quả 'đã học' (combiner), fallback heuristic nếu không có ----\n",
    "    COMBINER_ONLY = True  # theo yêu cầu: chỉ lấy 1 cái đã học\n",
    "    PRINT_FEATURE_BREAKDOWN = False\n",
    "\n",
    "    decision = None\n",
    "    learned = None\n",
    "\n",
    "    USE_COMBINER_IF_AVAILABLE = True\n",
    "    if USE_COMBINER_IF_AVAILABLE:\n",
    "        try:\n",
    "            import joblib\n",
    "            import warnings\n",
    "            warnings.filterwarnings('ignore', message='Trying to unpickle estimator .*')\n",
    "\n",
    "            def _find_combiner_joblib() -> Optional[pathlib.Path]:\n",
    "                candidates = []\n",
    "                candidates += [ROOT / 'combiner_logreg.joblib', ARTIFACT_DIR / 'combiner_logreg.joblib', pathlib.Path.cwd() / 'combiner_logreg.joblib']\n",
    "                if IS_KAGGLE:\n",
    "                    inp = pathlib.Path('/kaggle/input')\n",
    "                    if inp.exists():\n",
    "                        candidates += list(inp.rglob('combiner_logreg.joblib'))\n",
    "                for p in candidates:\n",
    "                    try:\n",
    "                        pp = pathlib.Path(p)\n",
    "                        if pp.exists():\n",
    "                            return pp\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                return None\n",
    "\n",
    "            comb_path = _find_combiner_joblib()\n",
    "            if comb_path is not None:\n",
    "                bundle = joblib.load(comb_path)\n",
    "                combiner = bundle.get('model', bundle) if isinstance(bundle, dict) else bundle\n",
    "                comb_cols = bundle.get('cols', None) if isinstance(bundle, dict) else None\n",
    "                if comb_cols:\n",
    "                    def _ensemble_decision_with_combiner(text: str) -> Dict[str, Any]:\n",
    "                        clf_entry = detector_predict(text, classifier_model, threshold=0.5, agg=DETECTOR_AGG, stride_tokens=CHUNK_STRIDE_TOKENS, max_windows=DETECTOR_MAX_WINDOWS)\n",
    "                        prob_model = float(clf_entry['prob_ai'])\n",
    "                        feat = compute_text_features(text)\n",
    "                        X_row = pd.DataFrame([{**feat, 'prob_model': prob_model}]).reindex(columns=comb_cols, fill_value=0.0)\n",
    "                        if COMBINER_DEBIAS_LENGTH:\n",
    "                            try:\n",
    "                                scaler = None\n",
    "                                if hasattr(combiner, 'steps'):\n",
    "                                    for _, est in (getattr(combiner, 'steps', []) or []):\n",
    "                                        if hasattr(est, 'mean_') and hasattr(est, 'scale_'):\n",
    "                                            scaler = est\n",
    "                                            break\n",
    "                                if scaler is not None and hasattr(scaler, 'mean_'):\n",
    "                                    means = dict(zip(list(comb_cols), list(np.asarray(scaler.mean_, dtype=float).ravel())))\n",
    "                                    for f in COMBINER_LENGTH_FEATURES:\n",
    "                                        if f in X_row.columns and f in means:\n",
    "                                            X_row.at[0, f] = float(means[f])\n",
    "                                else:\n",
    "                                    for f in COMBINER_LENGTH_FEATURES:\n",
    "                                        if f in X_row.columns:\n",
    "                                            X_row.at[0, f] = 0.0\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        combined = _combiner_prob_ai(combiner, X_row)\n",
    "                        return {\n",
    "                            'model': {**clf_entry},\n",
    "                            'combined_prob_ai': float(combined),\n",
    "                            'combined_label': 'AI' if float(combined) >= float(COMBINED_THRESHOLD) else 'Human',\n",
    "                            'combined_threshold': float(COMBINED_THRESHOLD),\n",
    "                        }\n",
    "                    learned = _ensemble_decision_with_combiner(SAMPLE_TEXT)\n",
    "        except Exception:\n",
    "            learned = None\n",
    "\n",
    "    if learned is not None:\n",
    "        print(json.dumps(learned, ensure_ascii=False, indent=2))\n",
    "    else:\n",
    "        decision = ensemble_decision(SAMPLE_TEXT, classifier_model, w_model=0.5)\n",
    "        print(json.dumps(decision, ensure_ascii=False, indent=2))\n",
    "        if PRINT_FEATURE_BREAKDOWN:\n",
    "            print('---- feature_ai_score_breakdown ----')\n",
    "            feat_dbg = compute_text_features(SAMPLE_TEXT)\n",
    "            print(json.dumps(feature_ai_score_breakdown(feat_dbg), ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print('⚠️ Không tìm thấy detector_phobert tại', eval_path if eval_path else ARTIFACT_DIR / 'detector_phobert')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
