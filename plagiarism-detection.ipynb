{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13833593,"sourceType":"datasetVersion","datasetId":8810220}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\n\nKAGGLE_INPUT_PATH = '/kaggle/input'\n\ndata_paths = {\n    'corpus': None,\n    'queries': None\n}\n\nif os.path.exists(KAGGLE_INPUT_PATH):\n    for root, dirs, files in os.walk(KAGGLE_INPUT_PATH):\n        for file in files:\n            if file == 'vn_plagiarism_corpus.json':\n                data_paths['corpus'] = os.path.join(root, file)\n            elif file == 'vn_plagiarism_queries.json':\n                data_paths['queries'] = os.path.join(root, file)\n\nif data_paths['corpus'] is None:\n    if os.path.exists('dataset_plagiarism_detection/vn_plagiarism_corpus.json'):\n        data_paths['corpus'] = 'dataset_plagiarism_detection/vn_plagiarism_corpus.json'\n    else:\n        data_paths['corpus'] = 'vn_plagiarism_corpus.json'\n        \nif data_paths['queries'] is None:\n    if os.path.exists('dataset_plagiarism_detection/vn_plagiarism_queries.json'):\n        data_paths['queries'] = 'dataset_plagiarism_detection/vn_plagiarism_queries.json'\n    else:\n        data_paths['queries'] = 'vn_plagiarism_queries.json'\n\nprint(\"Data file paths:\")\nprint(f\"   Corpus: {data_paths['corpus']}\")\nprint(f\"   Queries: {data_paths['queries']}\")\nprint(f\"   Corpus exists: {os.path.exists(data_paths['corpus'])}\")\nprint(f\"   Queries exists: {os.path.exists(data_paths['queries'])}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:45:57.058996Z","iopub.execute_input":"2025-11-23T01:45:57.059409Z","iopub.status.idle":"2025-11-23T01:45:57.074278Z","shell.execute_reply.started":"2025-11-23T01:45:57.059389Z","shell.execute_reply":"2025-11-23T01:45:57.073389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nprint(\"=\"*60)\nprint(\"LOADING VIETNAMESE PLAGIARISM DATASET\")\nprint(\"=\"*60)\n\ncorpus_data = None\nqueries_data = None\n\ntry:\n    with open(data_paths['corpus'], 'r', encoding='utf-8') as f:\n        corpus_data = json.load(f)\n    with open(data_paths['queries'], 'r', encoding='utf-8') as f:\n        queries_data = json.load(f)\n    \n    if corpus_data is None or queries_data is None:\n        raise ValueError(\"Data loaded but is None\")\n    \n    print(f\"Successfully loaded data!\")\n    print(f\"   Corpus: {len(corpus_data)} documents\")\n    print(f\"   Queries: {len(queries_data)} queries\")\n    \nexcept FileNotFoundError as e:\n    print(f\"ERROR: Data files not found!\")\n    print(f\"   {e}\")\n    print(f\"\\nInstructions:\")\n    print(f\"   1. Upload vn_plagiarism_corpus.json and vn_plagiarism_queries.json\")\n    print(f\"   2. Or add them as a Kaggle Dataset\")\n    print(f\"   3. Restart the kernel\")\n    raise\nexcept Exception as e:\n    print(f\"ERROR loading data: {e}\")\n    raise\n\nplagiarism_counts = {}\nfor query in queries_data:\n    ptype = query.get('plagiarism_type', 'unknown')\n    if ptype is None:\n        ptype = 'unknown'\n    plagiarism_counts[ptype] = plagiarism_counts.get(ptype, 0) + 1\n\nprint(f\"\\nPlagiarism type distribution:\")\nfor ptype, count in sorted(plagiarism_counts.items(), key=lambda x: x[1], reverse=True):\n    if ptype is None:\n        ptype = 'unknown'\n    is_plag = \"Plagiarism\" if ptype != \"original\" else \"Original\"\n    percentage = (count/len(queries_data)*100) if queries_data else 0\n    print(f\"   {is_plag:<12} | {ptype:<20} | {count:>4} ({percentage:>5.1f}%)\")\n\nquery_lengths = [len(q['text'].split()) for q in queries_data if 'text' in q]\ncorpus_lengths = [len(c['text'].split()) for c in corpus_data if 'text' in c]\n\nprint(f\"\\nText length statistics (words):\")\nprint(f\"   Query  - Min: {min(query_lengths):>4}, Max: {max(query_lengths):>4}, Avg: {np.mean(query_lengths):>6.1f}\")\nprint(f\"   Corpus - Min: {min(corpus_lengths):>4}, Max: {max(corpus_lengths):>4}, Avg: {np.mean(corpus_lengths):>6.1f}\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"Data ready for next step!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:46:14.131449Z","iopub.execute_input":"2025-11-23T01:46:14.131951Z","iopub.status.idle":"2025-11-23T01:46:16.852001Z","shell.execute_reply.started":"2025-11-23T01:46:14.131927Z","shell.execute_reply":"2025-11-23T01:46:16.851367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Installing required libraries...\")\n\n!pip install -q -U sentence-transformers\n!pip install -q faiss-cpu\n!pip install -q xgboost\n\nprint(\"Installation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:46:46.181825Z","iopub.execute_input":"2025-11-23T01:46:46.182427Z","iopub.status.idle":"2025-11-23T01:48:14.335772Z","shell.execute_reply.started":"2025-11-23T01:46:46.182399Z","shell.execute_reply":"2025-11-23T01:48:14.334853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport os\nimport re\nimport time\nimport warnings\n\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nimport faiss\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    accuracy_score, \n    precision_score, \n    recall_score, \n    f1_score\n)\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings('ignore')\n\nprint(\"Import thu vien thanh cong!\")\n\nplt.style.use('default')\nsns.set_palette('husl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:48:39.287952Z","iopub.execute_input":"2025-11-23T01:48:39.288265Z","iopub.status.idle":"2025-11-23T01:48:39.299474Z","shell.execute_reply.started":"2025-11-23T01:48:39.288247Z","shell.execute_reply":"2025-11-23T01:48:39.298826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"bkai-foundation-models/vietnamese-bi-encoder\"\n\nprint(\"=\"*60)\nprint(\"LOADING BI-ENCODER MODEL\")\nprint(\"=\"*60)\nprint(f\"Model: {model_name}\")\n\ntry:\n    bi_encoder = SentenceTransformer(model_name)\n    print(\"Load mo hinh thanh cong!\")\n    print(f\"   Max sequence length: {bi_encoder.max_seq_length}\")\n    print(f\"   Embedding dimension: {bi_encoder.get_sentence_embedding_dimension()}\")\nexcept Exception as e:\n    print(f\"Loi khi tai mo hinh '{model_name}': {e}\")\n    raise   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:49:12.20881Z","iopub.execute_input":"2025-11-23T01:49:12.209085Z","iopub.status.idle":"2025-11-23T01:49:21.91842Z","shell.execute_reply.started":"2025-11-23T01:49:12.209066Z","shell.execute_reply":"2025-11-23T01:49:21.917587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CHUNKING v√† l∆∞u l·∫°i \n\nimport re\nimport pickle\nimport os\n\nclass TextChunker:\n\n    \n    def __init__(self, chunk_type=\"adaptive\", max_chunk_words=100):\n        self.chunk_type = chunk_type  # \"sentence\", \"paragraph\", or \"adaptive\"\n        self.max_chunk_words = max_chunk_words  # Max words per chunk\n    \n    def chunk_text(self, text, doc_id):\n        \"\"\"\n        ADAPTIVE CHUNKING:\n        - Auto-detect best strategy based on text length\n        - Paragraph chunking for long docs (>500 words)\n        - Sentence chunking for short docs (<500 words)\n        \"\"\"\n        word_count = len(text.split())\n        \n        # Decide chunking strategy\n        if self.chunk_type == \"adaptive\":\n            # Long document ‚Üí Paragraph chunking\n            if word_count > 500:\n                return self._chunk_by_paragraph(text, doc_id)\n            # Short document ‚Üí Sentence chunking\n            else:\n                return self._chunk_by_sentence(text, doc_id)\n        \n        elif self.chunk_type == \"paragraph\":\n            return self._chunk_by_paragraph(text, doc_id)\n        \n        else:  # sentence\n            return self._chunk_by_sentence(text, doc_id)\n    \n    def _chunk_by_sentence(self, text, doc_id):\n        \"\"\"Original sentence chunking (fine-grained)\"\"\"\n        sentences = re.split(r'[.!?]+', text)\n        sentences = [s.strip() for s in sentences if s.strip()]\n        \n        chunks = []\n        for i, sentence in enumerate(sentences):\n            chunk = {\n                'chunk_id': f\"{doc_id}_chunk_{i}\",\n                'doc_id': doc_id,\n                'text': sentence,\n                'position': i,\n                'length': len(sentence.split())\n            }\n            chunks.append(chunk)\n        \n        return chunks\n    \n    def _chunk_by_paragraph(self, text, doc_id):\n        \"\"\"\n        PARAGRAPH CHUNKING (coarse-grained) - For LARGE corpus\n        \n        Strategy:\n        1. Split by double newline (\\n\\n) or multiple spaces\n        2. Merge small paragraphs (< 30 words)\n        3. Split large paragraphs (> max_chunk_words)\n        \"\"\"\n        \n        # Step 1: Split by paragraphs (double newline or 4+ spaces)\n        paragraphs = re.split(r'\\n\\n+|\\s{4,}', text)\n        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n        \n        # Step 2: Smart merging/splitting\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for para in paragraphs:\n            para_words = para.split()\n            para_length = len(para_words)\n            \n            # Case 1: Paragraph qu√° l·ªõn ‚Üí Split by sentences\n            if para_length > self.max_chunk_words:\n                # Flush current chunk\n                if current_chunk:\n                    chunks.append(' '.join(current_chunk))\n                    current_chunk = []\n                    current_length = 0\n                \n                # Split large paragraph by sentences\n                sentences = re.split(r'[.!?]+', para)\n                sentences = [s.strip() for s in sentences if s.strip()]\n                \n                temp_chunk = []\n                temp_length = 0\n                \n                for sent in sentences:\n                    sent_len = len(sent.split())\n                    \n                    if temp_length + sent_len > self.max_chunk_words and temp_chunk:\n                        chunks.append(' '.join(temp_chunk))\n                        temp_chunk = [sent]\n                        temp_length = sent_len\n                    else:\n                        temp_chunk.append(sent)\n                        temp_length += sent_len\n                \n                if temp_chunk:\n                    chunks.append(' '.join(temp_chunk))\n            \n            # Case 2: Merge v·ªõi chunk hi·ªán t·∫°i\n            elif current_length + para_length <= self.max_chunk_words:\n                current_chunk.append(para)\n                current_length += para_length\n            \n            # Case 3: Flush v√† start new chunk\n            else:\n                if current_chunk:\n                    chunks.append(' '.join(current_chunk))\n                current_chunk = [para]\n                current_length = para_length\n        \n        # Flush remaining\n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n        \n        # Step 3: Create chunk objects\n        chunk_objects = []\n        for i, chunk_text in enumerate(chunks):\n            chunk_objects.append({\n                'chunk_id': f\"{doc_id}_chunk_{i}\",\n                'doc_id': doc_id,\n                'text': chunk_text,\n                'position': i,\n                'length': len(chunk_text.split())\n            })\n        \n        return chunk_objects\n    \n    def chunk_corpus(self, corpus_data):\n        \"\"\"Chunk to√†n b·ªô corpus\"\"\"\n        all_chunks = []\n        \n        for doc in corpus_data:\n            doc_chunks = self.chunk_text(doc['text'], doc['id'])\n            all_chunks.extend(doc_chunks)\n        \n        return all_chunks\n\n# ===============================================\n# SAVE/LOAD CORPUS CHUNKS\n# ===============================================\nCORPUS_CHUNKS_FILE = 'corpus_chunks.pkl'\n\nif os.path.exists(CORPUS_CHUNKS_FILE):\n    print(\"=\"*60)\n    print(\"üìÇ LOADING SAVED CORPUS CHUNKS\")\n    print(\"=\"*60)\n    \n    start_time = time.time()\n    with open(CORPUS_CHUNKS_FILE, 'rb') as f:\n        corpus_chunks = pickle.load(f)\n    end_time = time.time()\n    \n    print(f\"‚úÖ Loaded {len(corpus_chunks)} chunks\")\n    print(f\"‚è±Ô∏è  Time: {end_time - start_time:.2f}s\")\n    print(f\"   Avg chunks/doc: {len(corpus_chunks)/len(corpus_data):.1f}\")\n    \nelse:\n    print(\"=\"*60)\n    print(\"üî® CREATING NEW CORPUS CHUNKS\")\n    print(\"=\"*60)\n    \n    # T·∫°o chunks\n    chunker = TextChunker()\n    corpus_chunks = chunker.chunk_corpus(corpus_data)\n    \n    print(f\"‚úÖ Created {len(corpus_chunks)} chunks from {len(corpus_data)} documents\")\n    print(f\"   Avg: {len(corpus_chunks)/len(corpus_data):.1f} chunks/doc\")\n    \n    # L∆∞u file\n    print(\"\\nüíæ Saving corpus chunks...\")\n    with open(CORPUS_CHUNKS_FILE, 'wb') as f:\n        pickle.dump(corpus_chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    file_size = os.path.getsize(CORPUS_CHUNKS_FILE) / 1024 / 1024\n    print(f\"‚úÖ Saved to {CORPUS_CHUNKS_FILE} (~{file_size:.1f} MB)\")\n\n# Create chunker instance for later use\nchunker = TextChunker()\n\n# Hi·ªÉn th·ªã m·∫´u\nprint(\"\\nüìù Sample chunks:\")\nfor i, chunk in enumerate(corpus_chunks[:3]):\n    print(f\"{i+1}. [{chunk['chunk_id']}] {chunk['text'][:60]}...\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ CORPUS CHUNKS READY!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:48:51.613829Z","iopub.execute_input":"2025-11-23T01:48:51.614119Z","iopub.status.idle":"2025-11-23T01:48:52.69077Z","shell.execute_reply.started":"2025-11-23T01:48:51.6141Z","shell.execute_reply":"2025-11-23T01:48:52.690074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#T·∫°o embeddings cho chunks \n\nimport os\nimport pickle\nimport numpy as np\nimport faiss\n\n# ===============================================\n# FILE PATHS\n# ===============================================\nEMBEDDINGS_FILE = 'chunk_embeddings_normalized.npy'\nFAISS_INDEX_FILE = 'chunk_faiss_index.faiss'\nCHUNK_METADATA_FILE = 'chunk_metadata.pkl'\n\n# ===============================================\n# HELPER: VALIDATE EMBEDDINGS\n# ===============================================\ndef validate_embeddings(embeddings, name=\"embeddings\"):\n    \"\"\"\n    Ki·ªÉm tra embeddings c√≥ h·ª£p l·ªá kh√¥ng\n    \n    Returns:\n        bool: True if valid, False otherwise\n    \"\"\"\n    # Check for NaN\n    nan_count = np.isnan(embeddings).sum()\n    if nan_count > 0:\n        print(f\"‚ùå ERROR: {name} contains {nan_count} NaN values!\")\n        return False\n    \n    # Check for Inf\n    inf_count = np.isinf(embeddings).sum()\n    if inf_count > 0:\n        print(f\"‚ùå ERROR: {name} contains {inf_count} Inf values!\")\n        return False\n    \n    # Check norm\n    norms = np.linalg.norm(embeddings, axis=1)\n    zero_norm_count = (norms < 1e-8).sum()\n    if zero_norm_count > 0:\n        print(f\"‚ö†Ô∏è  WARNING: {zero_norm_count} embeddings have near-zero norm!\")\n    \n    # Check range\n    min_val = embeddings.min()\n    max_val = embeddings.max()\n    print(f\"‚úÖ {name} validation:\")\n    print(f\"   Shape: {embeddings.shape}\")\n    print(f\"   Range: [{min_val:.6f}, {max_val:.6f}]\")\n    print(f\"   Norm: [{norms.min():.6f}, {norms.max():.6f}]\")\n    \n    return True\n\n# ===============================================\n# LOAD OR CREATE\n# ===============================================\nif (os.path.exists(EMBEDDINGS_FILE) and \n    os.path.exists(FAISS_INDEX_FILE) and \n    os.path.exists(CHUNK_METADATA_FILE)):\n    \n    print(\"=\"*60)\n    print(\"üìÇ LOADING SAVED EMBEDDINGS + FAISS INDEX\")\n    print(\"=\"*60)\n    \n    start_time = time.time()\n    \n    # Load embeddings\n    chunk_embeddings_normalized = np.load(EMBEDDINGS_FILE)\n    print(f\"‚úÖ Loaded embeddings: {chunk_embeddings_normalized.shape}\")\n    \n    # VALIDATE LOADED EMBEDDINGS\n    if not validate_embeddings(chunk_embeddings_normalized, \"Loaded embeddings\"):\n        print(\"\\n‚ö†Ô∏è  CORRUPTED FILE DETECTED!\")\n        print(\"   Deleting and recreating...\")\n        os.remove(EMBEDDINGS_FILE)\n        os.remove(FAISS_INDEX_FILE)\n        os.remove(CHUNK_METADATA_FILE)\n        raise ValueError(\"Corrupted files deleted. Please re-run this cell.\")\n    \n    # Load FAISS index\n    embedding_dim = chunk_embeddings_normalized.shape[1]\n    chunk_faiss_index = faiss.read_index(FAISS_INDEX_FILE)\n    print(f\"‚úÖ Loaded FAISS index: {chunk_faiss_index.ntotal} vectors\")\n    \n    # Load metadata\n    with open(CHUNK_METADATA_FILE, 'rb') as f:\n        metadata = pickle.load(f)\n    chunk_ids = metadata['chunk_ids']\n    print(f\"‚úÖ Loaded metadata: {len(chunk_ids)} chunk IDs\")\n    \n    # TEST FAISS INDEX\n    print(f\"\\nüß™ Testing FAISS index...\")\n    test_query = chunk_embeddings_normalized[0:1].astype('float32')\n    test_similarities, test_indices = chunk_faiss_index.search(test_query, k=5)\n    \n    if np.isfinite(test_similarities).all():\n        print(f\"‚úÖ FAISS test passed! Similarity range: [{test_similarities[0].min():.6f}, {test_similarities[0].max():.6f}]\")\n    else:\n        print(f\"‚ùå FAISS index CORRUPTED! Deleting...\")\n        os.remove(EMBEDDINGS_FILE)\n        os.remove(FAISS_INDEX_FILE)\n        os.remove(CHUNK_METADATA_FILE)\n        raise ValueError(\"Corrupted FAISS index deleted. Please re-run this cell.\")\n    \n    end_time = time.time()\n    print(f\"\\n‚è±Ô∏è  Loading time: {end_time - start_time:.2f}s\")\n    print(f\"üöÄ Ready!\")\n    \nelse:\n    print(\"=\"*60)\n    print(\"üî® CREATING NEW EMBEDDINGS + FAISS INDEX\")\n    print(\"=\"*60)\n    \n    # Extract texts\n    chunk_texts = [chunk['text'] for chunk in corpus_chunks]\n    chunk_ids = [chunk['chunk_id'] for chunk in corpus_chunks]\n    \n    print(f\"üìä Creating embeddings for {len(chunk_texts)} chunks...\")\n    \n    # Create embeddings\n    start_time = time.time()\n    chunk_embeddings = bi_encoder.encode(\n        chunk_texts,\n        show_progress_bar=True,\n        batch_size=32,\n        convert_to_numpy=True\n    )\n    \n    print(f\"\\nüîç Validating raw embeddings...\")\n    if not validate_embeddings(chunk_embeddings, \"Raw embeddings\"):\n        raise ValueError(\"Raw embeddings are invalid! Check bi_encoder model.\")\n    \n    # Normalize (WITH SAFETY CHECK)\n    print(f\"\\nüîÑ Normalizing embeddings...\")\n    norms = np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)\n    \n    # Replace zero norms with 1.0 to avoid division by zero\n    zero_norm_mask = (norms < 1e-8)\n    if zero_norm_mask.any():\n        print(f\"‚ö†Ô∏è  Found {zero_norm_mask.sum()} zero-norm embeddings, fixing...\")\n        norms[zero_norm_mask] = 1.0\n    \n    chunk_embeddings_normalized = chunk_embeddings / norms\n    \n    # Validate normalized embeddings\n    print(f\"\\nüîç Validating normalized embeddings...\")\n    if not validate_embeddings(chunk_embeddings_normalized, \"Normalized embeddings\"):\n        raise ValueError(\"Normalized embeddings are invalid!\")\n    \n    end_time = time.time()\n    print(f\"\\n‚úÖ Embeddings created and validated!\")\n    print(f\"   Time: {end_time - start_time:.2f}s\")\n    \n    # Create FAISS index\n    print(\"\\nüîß Creating FAISS index...\")\n    embedding_dim = chunk_embeddings_normalized.shape[1]\n    \n    # Use IndexFlatIP for small corpus\n    chunk_faiss_index = faiss.IndexFlatIP(embedding_dim)\n    \n    # Add vectors (convert to float32 explicitly)\n    vectors_to_add = chunk_embeddings_normalized.astype('float32')\n    \n    # Final validation before adding\n    if not np.isfinite(vectors_to_add).all():\n        raise ValueError(\"Non-finite values detected before adding to FAISS!\")\n    \n    chunk_faiss_index.add(vectors_to_add)\n    \n    print(f\"‚úÖ FAISS index created: {chunk_faiss_index.ntotal} vectors\")\n    \n    # TEST FAISS INDEX\n    print(\"\\nüß™ Testing FAISS index...\")\n    test_query = chunk_embeddings_normalized[0:1].astype('float32')\n    test_similarities, test_indices = chunk_faiss_index.search(test_query, k=5)\n    \n    print(f\"   Test similarities: {test_similarities[0][:5]}\")\n    \n    if np.isfinite(test_similarities).all():\n        print(f\"‚úÖ FAISS index test PASSED!\")\n    else:\n        print(f\"‚ùå FAISS index test FAILED!\")\n        raise ValueError(\"FAISS index is corrupted!\")\n    \n    # Save everything\n    print(\"\\nüíæ Saving to disk...\")\n    \n    # Save embeddings\n    np.save(EMBEDDINGS_FILE, chunk_embeddings_normalized)\n    emb_size = os.path.getsize(EMBEDDINGS_FILE) / 1024 / 1024\n    print(f\"‚úÖ Saved: {EMBEDDINGS_FILE} (~{emb_size:.1f} MB)\")\n    \n    # Save FAISS index\n    faiss.write_index(chunk_faiss_index, FAISS_INDEX_FILE)\n    faiss_size = os.path.getsize(FAISS_INDEX_FILE) / 1024 / 1024\n    print(f\"‚úÖ Saved: {FAISS_INDEX_FILE} (~{faiss_size:.1f} MB)\")\n    \n    # Save metadata\n    metadata = {\n        'chunk_ids': chunk_ids,\n        'num_chunks': len(chunk_ids),\n        'embedding_dim': embedding_dim,\n        'model_name': 'bkai-foundation-models/vietnamese-bi-encoder'\n    }\n    with open(CHUNK_METADATA_FILE, 'wb') as f:\n        pickle.dump(metadata, f)\n    print(f\"‚úÖ Saved: {CHUNK_METADATA_FILE}\")\n    \n    print(f\"\\nüéâ All files saved! Next run will be faster!\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ CHUNK EMBEDDINGS + FAISS INDEX READY!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:49:27.509522Z","iopub.execute_input":"2025-11-23T01:49:27.510247Z","iopub.status.idle":"2025-11-23T01:52:30.632353Z","shell.execute_reply.started":"2025-11-23T01:49:27.51022Z","shell.execute_reply":"2025-11-23T01:52:30.63156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Document scoring v·ªõi chu·∫©n h√≥a m·ªÅm ƒë·ªÉ gi·∫£m b√£o h√≤a ƒëi·ªÉm\nclass DocumentScorer:\n    def __init__(self, corpus_chunks, weights=None, score_center=0.55, score_scale=4.0):\n        self.corpus_chunks = corpus_chunks\n        self.weights = weights or {\n            'doc_max': 0.40,\n            'doc_mean': 0.20,\n            'doc_count': 0.15,\n            'doc_contiguous': 0.10,\n            'doc_coverage': 0.10,\n            'chunk_density': 0.05,\n            'span_penalty': 0.05\n        }\n        self.score_center = score_center\n        self.score_scale = score_scale\n        self.chunk_map = {chunk['chunk_id']: chunk for chunk in corpus_chunks}\n        self.doc_chunks_map = {}\n        for chunk in corpus_chunks:\n            self.doc_chunks_map.setdefault(chunk['doc_id'], []).append(chunk)\n\n    def calculate_doc_scores(self, top_k_results):\n        doc_similarities = {}\n        for similarity, chunk_idx in top_k_results:\n            chunk = self.corpus_chunks[chunk_idx]\n            doc_id = chunk['doc_id']\n            doc_similarities.setdefault(doc_id, []).append({\n                'similarity': similarity,\n                'chunk': chunk,\n                'chunk_idx': chunk_idx\n            })\n\n        doc_scores = []\n        for doc_id, chunk_sims in doc_similarities.items():\n            similarities = [cs['similarity'] for cs in chunk_sims]\n            doc_max = max(similarities)\n            doc_mean = np.mean(similarities)\n            total_doc_chunks = len(self.doc_chunks_map[doc_id])\n            coverage_ratio = min(len(similarities) / total_doc_chunks, 1.0)\n            doc_count = coverage_ratio  # gi·ªØ t∆∞∆°ng th√≠ch t√™n c≈©\n\n            positions = sorted(cs['chunk']['position'] for cs in chunk_sims)\n            doc_contiguous, max_group_len = self._calculate_contiguous_score(positions)\n            span = positions[-1] - positions[0] + 1 if len(positions) > 1 else 1\n            span_ratio = min(span / total_doc_chunks, 1.0)\n            chunk_density = min(coverage_ratio / max(span_ratio, 1e-6), 1.0)\n            chunk_similarity_std = float(np.std(similarities)) if len(similarities) > 1 else 0.0\n\n            raw_score = (\n                self.weights['doc_max'] * doc_max +\n                self.weights['doc_mean'] * doc_mean +\n                self.weights['doc_count'] * doc_count +\n                self.weights['doc_contiguous'] * doc_contiguous +\n                self.weights['doc_coverage'] * coverage_ratio +\n                self.weights['chunk_density'] * chunk_density -\n                self.weights['span_penalty'] * (1.0 - span_ratio)\n            )\n\n            logistic_input = self.score_scale * (raw_score - self.score_center)\n            final_score = 1.0 / (1.0 + np.exp(-logistic_input))\n\n            doc_scores.append({\n                'doc_id': doc_id,\n                'doc_max': doc_max,\n                'doc_mean': doc_mean,\n                'doc_count': doc_count,\n                'doc_contiguous': doc_contiguous,\n                'final_score': final_score,\n                'raw_score': raw_score,\n                'chunk_similarity_std': chunk_similarity_std,\n                'position_span_ratio': span_ratio,\n                'chunk_density': chunk_density,\n                'contiguous_len': max_group_len,\n                'chunks': chunk_sims,\n                'num_chunks': len(chunk_sims)\n            })\n\n        doc_scores.sort(key=lambda x: x['final_score'], reverse=True)\n        return doc_scores\n\n    def _calculate_contiguous_score(self, positions):\n        if len(positions) <= 1:\n            return 0.0, len(positions)\n        contiguous_groups = []\n        current_group = [positions[0]]\n        for i in range(1, len(positions)):\n            if positions[i] - positions[i-1] <= 2:\n                current_group.append(positions[i])\n            else:\n                contiguous_groups.append(len(current_group))\n                current_group = [positions[i]]\n        contiguous_groups.append(len(current_group))\n        max_contiguous = max(contiguous_groups)\n        score = min(max_contiguous / len(positions), 1.0)\n        return score, max_contiguous\n\n\ndoc_scorer = DocumentScorer(corpus_chunks)\nprint(\"Document Scorer ready!\")\nprint(f\" Weights: {doc_scorer.weights}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:52:33.403445Z","iopub.execute_input":"2025-11-23T01:52:33.403995Z","iopub.status.idle":"2025-11-23T01:52:33.429742Z","shell.execute_reply.started":"2025-11-23T01:52:33.403971Z","shell.execute_reply":"2025-11-23T01:52:33.428956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" \n# BI-ENCODER PIPELINE\n# M·ª•c ƒë√≠ch: T√¨m top 100 corpus chunks gi·ªëng nh·∫•t v·ªõi c√°c chunk c·ªßa query\n\ndef bi_encoder_search_chunked(query_text, query_id=\"query_temp\", top_k=100):\n    \n    # B∆∞·ªõc 1: Chia query th√†nh chunks\n    query_chunks = chunker.chunk_text(query_text, doc_id=query_id)\n    print(f\"   Query ƒë∆∞·ª£c chia th√†nh {len(query_chunks)} chunks\")\n    \n    # B∆∞·ªõc 2: T·∫°o embeddings cho T·∫§T C·∫¢ query chunks\n    query_texts = [chunk['text'] for chunk in query_chunks]\n    query_embeddings = bi_encoder.encode(query_texts, convert_to_numpy=True)\n    \n    # Normalize query embeddings\n    query_norms = np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n    query_norms[query_norms < 1e-8] = 1.0  # Tr√°nh chia cho 0\n    query_embeddings_normalized = query_embeddings / query_norms\n    \n    # VALIDATE query embeddings\n    if not np.isfinite(query_embeddings_normalized).all():\n        print(f\"   ‚ö†Ô∏è Some query chunks have invalid embeddings!\")\n        # Lo·∫°i b·ªè query chunks c√≥ invalid embeddings\n        valid_mask = np.isfinite(query_embeddings_normalized).all(axis=1)\n        query_embeddings_normalized = query_embeddings_normalized[valid_mask]\n        print(f\"   Keeping {valid_mask.sum()}/{len(query_chunks)} valid query chunks\")\n    \n    if len(query_embeddings_normalized) == 0:\n        print(f\"   ‚ùå No valid query embeddings!\")\n        return []\n    \n    # B∆∞·ªõc 3: T√≠nh similarity gi·ªØa T·ª™NG corpus chunk v·ªõi T·∫§T C·∫¢ query chunks\n    # Shape: (num_query_chunks, num_corpus_chunks)\n    similarity_matrix = np.dot(\n        query_embeddings_normalized.astype('float32'),\n        chunk_embeddings_normalized.T.astype('float32')\n    )\n    \n    # VALIDATE similarity matrix\n    if not np.isfinite(similarity_matrix).all():\n        print(f\"   ‚ö†Ô∏è Similarity matrix contains invalid values!\")\n        similarity_matrix = np.nan_to_num(similarity_matrix, nan=0.0, posinf=1.0, neginf=0.0)\n    \n    # B∆∞·ªõc 4: Aggregate similarity cho m·ªói corpus chunk\n    # Strategy: L·∫•y MAX similarity v·ªõi b·∫•t k·ª≥ query chunk n√†o\n    # (v√¨ n·∫øu corpus chunk gi·ªëng v·ªõi B·∫§T K·ª≤ ph·∫ßn n√†o c·ªßa query ‚Üí c√≥ kh·∫£ nƒÉng plagiarism)\n    corpus_scores = np.max(similarity_matrix, axis=0)  # Shape: (num_corpus_chunks,)\n    \n    print(f\"   Computed similarity v·ªõi {len(corpus_scores)} corpus chunks\")\n    print(f\"   Similarity range: [{corpus_scores.min():.6f}, {corpus_scores.max():.6f}]\")\n    \n    # B∆∞·ªõc 5: L·∫•y top-K corpus chunks\n    top_k_actual = min(top_k, len(corpus_scores))\n    top_k_indices = np.argsort(corpus_scores)[::-1][:top_k_actual]\n    \n    # Convert to results format: [(similarity, chunk_idx), ...]\n    results = [(float(corpus_scores[idx]), int(idx)) for idx in top_k_indices]\n    \n    print(f\"   Selected top {len(results)} corpus chunks\")\n    \n    return results\n\n# Test v·ªõi m·ªôt query\ntest_query = queries_data[33]\nprint(f\"üß™ TEST BI-ENCODER PIPELINE WITH QUERY CHUNKING\")\nprint(f\"=\"*60)\nprint(f\"Query ID: {test_query['id']}\")\nprint(f\"Query: {test_query['text'][:100]}...\")\nprint(f\"Query length: {len(test_query['text'].split())} words\")\nprint(f\"True label: {'PLAGIARISM' if test_query['is_plagiarism'] else 'ORIGINAL'}\")\nprint(f\"True type: {test_query['plagiarism_type']}\")\n\n# Bi-encoder search with chunking (t√¨m top-100 corpus chunks gi·ªëng nh·∫•t v·ªõi to√†n b·ªô query)\nprint(f\"\\nüîç Chunking query and searching top 100 similar corpus chunks...\")\nbi_results = bi_encoder_search_chunked(test_query['text'], query_id=test_query['id'], top_k=100)\n\nif len(bi_results) > 0:\n    print(f\"\\n‚úÖ Found {len(bi_results)} unique chunks\")\n    print(f\"   Similarity range: [{bi_results[-1][0]:.6f}, {bi_results[0][0]:.6f}]\")\n    \n    # Document scoring\n    doc_scores = doc_scorer.calculate_doc_scores(bi_results)\n    print(f\"\\nüìä Document scores (Top 10):\")\n    print(\"=\"*60)\n    \n    for i, doc_score in enumerate(doc_scores[:10]):\n        print(f\"{i+1}. Doc: {doc_score['doc_id']} | Chunks: {doc_score['num_chunks']}\")\n        print(f\"    Final: {doc_score['final_score']:.3f}\")\n        print(f\"    Max: {doc_score['doc_max']:.3f} | Mean: {doc_score['doc_mean']:.3f}\")\n        print(f\"    Count: {doc_score['doc_count']:.3f} | Contiguous: {doc_score['doc_contiguous']:.3f}\")\n        print()\n    \n    print(f\"üéØ Best match: {doc_scores[0]['doc_id']} (score: {doc_scores[0]['final_score']:.3f})\")\n    \n    # Ki·ªÉm tra xem c√≥ ƒë√∫ng source document kh√¥ng (n·∫øu l√† plagiarism)\n    if test_query['is_plagiarism'] and test_query.get('source_doc_id'):\n        source_rank = None\n        for i, doc_score in enumerate(doc_scores):\n            if doc_score['doc_id'] == test_query['source_doc_id']:\n                source_rank = i + 1\n                break\n        \n        if source_rank:\n            print(f\"‚úÖ Source document {test_query['source_doc_id']} found at rank {source_rank}\")\n        else:\n            print(f\"‚ùå Source document {test_query['source_doc_id']} NOT in top results\")\n    else:\n        print(f\"‚ÑπÔ∏è  This is an original text (no plagiarism)\")\nelse:\n    print(f\"‚ùå No valid results found!\")\n\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:52:37.128896Z","iopub.execute_input":"2025-11-23T01:52:37.129422Z","iopub.status.idle":"2025-11-23T01:52:37.378234Z","shell.execute_reply.started":"2025-11-23T01:52:37.129398Z","shell.execute_reply":"2025-11-23T01:52:37.377442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# M·ªû R·ªòNG NG·ªÆ C·∫¢NH CHO CHUNKS\n# (Gi·ªØ l·∫°i ƒë·ªÉ tra c·ª©u ng·ªØ c·∫£nh chunk khi c·∫ßn debug, nh∆∞ng kh√¥ng c√≤n ph·ª•c v·ª• cross-encoder)\nclass ContextExpander:\n    def __init__(self, corpus_chunks, corpus_data):\n        self.corpus_chunks = corpus_chunks\n        self.corpus_data = corpus_data\n        \n        self.doc_text_map = {doc['id']: doc['text'] for doc in corpus_data}\n        self.doc_chunks_map = {}\n        for chunk in corpus_chunks:\n            doc_id = chunk['doc_id']\n            self.doc_chunks_map.setdefault(doc_id, []).append(chunk)\n        for doc_id in self.doc_chunks_map:\n            self.doc_chunks_map[doc_id].sort(key=lambda x: x['position'])\n    \n    def expand_chunk_context(self, chunk, context_window=1):\n        doc_id = chunk['doc_id']\n        position = chunk['position']\n        doc_chunks = self.doc_chunks_map[doc_id]\n        current_idx = next((idx for idx, c in enumerate(doc_chunks) if c['position'] == position), None)\n        if current_idx is None:\n            return chunk['text']\n        start_idx = max(0, current_idx - context_window)\n        end_idx = min(len(doc_chunks), current_idx + context_window + 1)\n        context_chunks = doc_chunks[start_idx:end_idx]\n        return \" \".join([c['text'] for c in context_chunks])\n    \n    def get_best_chunks_per_doc(self, doc_scores, top_n_docs=10, chunks_per_doc=1):\n        best_chunks = []\n        for doc_score in doc_scores[:top_n_docs]:\n            sorted_chunks = sorted(doc_score['chunks'], key=lambda x: x['similarity'], reverse=True)\n            selected_chunks = sorted_chunks if chunks_per_doc == -1 else sorted_chunks[:chunks_per_doc]\n            for chunk_data in selected_chunks:\n                best_chunks.append({\n                    'doc_id': doc_score['doc_id'],\n                    'chunk': chunk_data['chunk'],\n                    'chunk_similarity': chunk_data['similarity'],\n                    'doc_final_score': doc_score['final_score'],\n                    'doc_max': doc_score['doc_max'],\n                    'doc_mean': doc_score['doc_mean'],\n                    'doc_count': doc_score['doc_count'],\n                    'doc_contiguous': doc_score['doc_contiguous']\n                })\n        return best_chunks\n    \n    def get_best_chunk_per_doc(self, doc_scores, top_n=15):\n        return self.get_best_chunks_per_doc(doc_scores, top_n_docs=top_n, chunks_per_doc=1)\n\ncontext_expander = ContextExpander(corpus_chunks, corpus_data)\nprint(\"‚úÖ Context Expander ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:53:06.764647Z","iopub.execute_input":"2025-11-23T01:53:06.765315Z","iopub.status.idle":"2025-11-23T01:53:06.783102Z","shell.execute_reply.started":"2025-11-23T01:53:06.765287Z","shell.execute_reply":"2025-11-23T01:53:06.782305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass CompletePlagiarismDetector:\n    def __init__(self, bi_encoder, chunk_faiss_index, corpus_chunks, corpus_data,\n                 doc_scorer, context_expander,\n                 query_chunker=None, max_query_chunks=10, threshold=0.7):\n        self.bi_encoder = bi_encoder\n        self.chunk_faiss_index = chunk_faiss_index  # optional, not required for dot-product path\n        self.corpus_chunks = corpus_chunks\n        self.corpus_data = corpus_data\n        self.doc_scorer = doc_scorer\n        self.context_expander = context_expander\n        self.query_chunker = query_chunker or TextChunker()\n        self.max_query_chunks = max_query_chunks\n        self.threshold = threshold\n\n    def detect(self, query_text, top_k=100, top_n_docs=15, use_faiss=False, verbose=False):\n        if verbose:\n            print(\"=\"*60)\n            print(\"PLAGIARISM DETECTION PIPELINE (BI-ENCODER ONLY)\")\n            print(\"=\"*60)\n\n        # Step 1: chunk query\n        query_chunks = self.query_chunker.chunk_text(query_text, doc_id=\"query\")\n        if self.max_query_chunks and len(query_chunks) > self.max_query_chunks:\n            query_chunks = query_chunks[:self.max_query_chunks]\n        query_texts = [c['text'] for c in query_chunks]\n        word_count = len(query_text.split())\n\n        if len(query_texts) == 0:\n            return {\n                'prediction': False,\n                'confidence': 0.0,\n                'threshold': self.threshold,\n                'best_match': None,\n                'top_results': [],\n                'doc_scores': [],\n                'method': 'bi-encoder',\n                'query_chunks': 0,\n                'query_words': word_count,\n                'corpus_matches': 0\n            }\n\n        if verbose:\n            print(f\"Step 1: Query words: {word_count}, chunks: {len(query_texts)}\")\n\n        # Step 2: embed query chunks\n        q_emb = self.bi_encoder.encode(query_texts, show_progress_bar=False, convert_to_numpy=True)\n        q_norms = np.linalg.norm(q_emb, axis=1, keepdims=True)\n        q_norms[q_norms < 1e-8] = 1.0\n        q_emb_norm = q_emb / q_norms\n\n        if use_faiss and hasattr(self, 'chunk_faiss_index') and self.chunk_faiss_index is not None:\n            # per-chunk FAISS search aggregation (safe)\n            corpus_chunk_scores = {}\n            per_chunk_k = min(top_k, self.chunk_faiss_index.ntotal)\n            for i, qv in enumerate(q_emb_norm):\n                if not np.isfinite(qv).all():\n                    continue\n                sims, idxs = self.chunk_faiss_index.search(qv.reshape(1, -1).astype('float32'), per_chunk_k)\n                sims = sims[0]; idxs = idxs[0]\n                # sanitize\n                sims = np.nan_to_num(sims, nan=0.0, posinf=1.0, neginf=0.0)\n                for j, corpus_idx in enumerate(idxs):\n                    sim = float(sims[j])\n                    if not np.isfinite(sim):\n                        continue\n                    if corpus_idx not in corpus_chunk_scores:\n                        corpus_chunk_scores[corpus_idx] = {'max_similarity': sim, 'query_chunks':[i]}\n                    else:\n                        if sim > corpus_chunk_scores[corpus_idx]['max_similarity']:\n                            corpus_chunk_scores[corpus_idx]['max_similarity'] = sim\n                        if i not in corpus_chunk_scores[corpus_idx]['query_chunks']:\n                            corpus_chunk_scores[corpus_idx]['query_chunks'].append(i)\n            bi_results = [(v['max_similarity'], int(k)) for k, v in corpus_chunk_scores.items()]\n            bi_results.sort(reverse=True, key=lambda x: x[0])\n            bi_results = bi_results[:top_k]\n        else:\n            # dot-product matrix path (fast if chunk_embeddings_normalized is in memory)\n            # requires chunk_embeddings_normalized to exist in notebook scope\n            similarity_matrix = np.dot(q_emb_norm.astype('float32'), chunk_embeddings_normalized.T.astype('float32'))\n            if not np.isfinite(similarity_matrix).all():\n                similarity_matrix = np.nan_to_num(similarity_matrix, nan=0.0, posinf=1.0, neginf=0.0)\n            corpus_scores = np.max(similarity_matrix, axis=0)  # max over query chunks\n            top_k_actual = min(top_k, len(corpus_scores))\n            top_k_indices = np.argsort(corpus_scores)[::-1][:top_k_actual]\n            bi_results = [(float(corpus_scores[idx]), int(idx)) for idx in top_k_indices]\n\n        if verbose:\n            if len(bi_results) > 0:\n                print(f\"Step 2: Selected top {len(bi_results)} corpus chunks | sim range [{bi_results[-1][0]:.6f}, {bi_results[0][0]:.6f}]\")\n            else:\n                print(\"Step 2: No matching corpus chunks found\")\n\n        # Step 3: document scoring\n        doc_scores = self.doc_scorer.calculate_doc_scores(bi_results)\n        if verbose:\n            print(f\"Step 3: Scored {len(doc_scores)} documents\")\n\n        # Step 4: select best chunk per doc\n        best_chunks = self.context_expander.get_best_chunk_per_doc(doc_scores, top_n=top_n_docs)\n        if verbose:\n            print(f\"Step 4: Selected {len(best_chunks)} best chunks from top-{top_n_docs} docs\")\n\n        # Final: use document final_score as confidence\n        best_doc = doc_scores[0] if len(doc_scores) > 0 else None\n        confidence = float(best_doc['final_score']) if best_doc is not None else 0.0\n        is_plagiarism = confidence >= self.threshold\n\n        if verbose:\n            print(\"\\n\" + \"=\"*60)\n            print(\"‚≠ê FINAL RESULT:\")\n            print(f\"   Prediction: {'PLAGIARISM' if is_plagiarism else 'ORIGINAL'}\")\n            print(f\"   Confidence: {confidence:.3f}\")\n            print(f\"   Threshold: {self.threshold}\")\n            if best_doc:\n                print(f\"   Best match doc: {best_doc['doc_id']} (final_score: {best_doc['final_score']:.3f})\")\n            print(\"=\"*60)\n\n        return {\n            'prediction': bool(is_plagiarism),\n            'confidence': confidence,\n            'threshold': self.threshold,\n            'best_match': best_doc,\n            'top_results': doc_scores[:top_n_docs],\n            'doc_scores': doc_scores[:5],\n            'method': 'bi-encoder',\n            'query_chunks': len(query_chunks),\n            'query_words': word_count,\n            'corpus_matches': len(bi_results)\n        }\n\n\nprint(\"=\"*60)\nprint(\"CREATING PLAGIARISM DETECTOR\")\nprint(\"=\"*60)\n\ncomplete_detector_v2 = CompletePlagiarismDetector(\n    bi_encoder=bi_encoder,\n    chunk_faiss_index=chunk_faiss_index,\n    corpus_chunks=corpus_chunks,\n    corpus_data=corpus_data,\n    doc_scorer=doc_scorer,\n    context_expander=context_expander,\n    query_chunker=chunker,\n    max_query_chunks=10,\n    threshold=0.6   # adjust as needed\n)\n\nprint(\"‚úÖ Complete Plagiarism Detector (bi-encoder) ready!\")\nprint(f\"\\nüìä Configuration:\")\nprint(f\"   Threshold: {complete_detector_v2.threshold}\")\nprint(f\"   Max query chunks: {complete_detector_v2.max_query_chunks}\")\nprint(f\"   Method: {complete_detector_v2.__class__.__name__} -> bi-encoder only\")\n# ...existing code...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:53:10.195467Z","iopub.execute_input":"2025-11-23T01:53:10.195743Z","iopub.status.idle":"2025-11-23T01:53:10.215797Z","shell.execute_reply.started":"2025-11-23T01:53:10.195723Z","shell.execute_reply":"2025-11-23T01:53:10.214832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B∆Ø·ªöC 13A: ML CLASSIFIER CLASS\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pickle\nimport os\n\nclass MLClassifier:\n    \"\"\"\n    Machine Learning Classifier cho Plagiarism Detection\n    T·∫≠n d·ª•ng ƒë·∫∑c tr∆∞ng doc-level t·ª´ bi-encoder/doc scoring + th·ªëng k√™ coverage.\n    \"\"\"\n\n    def __init__(self, model_type='xgboost'):\n        self.model_type = model_type\n        if model_type == 'xgboost':\n            self.model = XGBClassifier(\n                n_estimators=120,\n                max_depth=6,\n                learning_rate=0.1,\n                subsample=0.9,\n                colsample_bytree=0.9,\n                random_state=42,\n                eval_metric='logloss'\n            )\n        else:\n            raise ValueError(f\"Model type {model_type} not supported\")\n\n        self.feature_names = [\n            'best_final',\n            'raw_score',\n            'doc_max',\n            'doc_mean',\n            'doc_count',\n            'doc_contiguous',\n            'chunk_similarity_std',\n            'position_span_ratio',\n            'chunk_density',\n            'top_final_std',\n            'top_final_gap',\n            'best_chunks_ratio'\n        ]\n        self.is_trained = False\n\n    def extract_features(self, result):\n        best_match = result.get('best_match')\n        top_results = result.get('top_results', []) or []\n        if not best_match or len(top_results) == 0:\n            return [0.0] * len(self.feature_names)\n\n        best_final = float(best_match.get('final_score', 0.0))\n        raw_score = float(best_match.get('raw_score', best_final))\n        doc_max = float(best_match.get('doc_max', 0.0))\n        doc_mean = float(best_match.get('doc_mean', 0.0))\n        doc_count = float(best_match.get('doc_count', 0.0))\n        doc_contiguous = float(best_match.get('doc_contiguous', 0.0))\n        chunk_similarity_std = float(best_match.get('chunk_similarity_std', 0.0))\n        position_span_ratio = float(best_match.get('position_span_ratio', 0.0))\n        chunk_density = float(best_match.get('chunk_density', 0.0))\n\n        final_scores = [float(d.get('final_score', 0.0)) for d in top_results]\n        top_final_std = float(np.std(final_scores)) if len(final_scores) > 1 else 0.0\n        top_final_gap = float(final_scores[0] - final_scores[1]) if len(final_scores) > 1 else 0.0\n\n        best_num_chunks = float(best_match.get('num_chunks', 0.0))\n        total_top_chunks = float(sum(d.get('num_chunks', 0.0) for d in top_results))\n        if total_top_chunks <= 0:\n            total_top_chunks = 1.0\n        best_chunks_ratio = float(best_num_chunks / total_top_chunks)\n\n        features = [\n            best_final,\n            raw_score,\n            doc_max,\n            doc_mean,\n            doc_count,\n            doc_contiguous,\n            chunk_similarity_std,\n            position_span_ratio,\n            chunk_density,\n            top_final_std,\n            top_final_gap,\n            best_chunks_ratio\n        ]\n        return features\n\n    def train(self, detector, queries_data, test_size=0.2, verbose=True):\n        if verbose:\n            print(f\"Extracting features t·ª´ {len(queries_data)} queries...\")\n        X, y = [], []\n        for i, query in enumerate(queries_data):\n            if verbose and (i + 1) % 20 == 0:\n                print(f\"   Progress: {i+1}/{len(queries_data)}\")\n            result = detector.detect(query['text'], verbose=False)\n            X.append(self.extract_features(result))\n            y.append(query['is_plagiarism'])\n\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=42, stratify=y\n        )\n        if verbose:\n            print(f\"\\nüìä Dataset split:\")\n            print(f\"   Train: {len(X_train)} samples\")\n            print(f\"   Test:  {len(X_test)} samples\")\n            print(f\"\\nüî® Training {self.model_type.upper()} classifier...\")\n\n        self.model.fit(X_train, y_train)\n\n        train_pred = self.model.predict(X_train)\n        test_pred = self.model.predict(X_test)\n        train_acc = accuracy_score(y_train, train_pred)\n        test_acc = accuracy_score(y_test, test_pred)\n        train_f1 = f1_score(y_train, train_pred)\n        test_f1 = f1_score(y_test, test_pred)\n        self.is_trained = True\n\n        if verbose:\n            print(f\"\\n‚úÖ Training completed!\")\n            print(f\"   Train Accuracy: {train_acc*100:.2f}% | F1: {train_f1*100:.2f}%\")\n            print(f\"   Test  Accuracy: {test_acc*100:.2f}% | F1: {test_f1*100:.2f}%\")\n\n        if hasattr(self.model, 'feature_importances_'):\n            print(f\"\\n‚≠ê Feature Importance:\")\n            for name, imp in sorted(zip(self.feature_names, self.model.feature_importances_),\n                                    key=lambda x: x[1], reverse=True):\n                print(f\"   {name:<20} {imp:>6.3f}\")\n\n        return {\n            'train_accuracy': train_acc,\n            'test_accuracy': test_acc,\n            'train_f1': train_f1,\n            'test_f1': test_f1,\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test,\n            'train_pred': train_pred,\n            'test_pred': test_pred\n        }\n\n    def predict(self, result):\n        if not self.is_trained:\n            raise ValueError(\"Model ch∆∞a ƒë∆∞·ª£c train! Ch·∫°y .train() tr∆∞·ªõc.\")\n        features = self.extract_features(result)\n        prediction = self.model.predict([features])[0]\n        confidence = self.model.predict_proba([features])[0][1]\n        return {\n            'prediction': bool(prediction),\n            'confidence': float(confidence),\n            'method': 'ml_classifier',\n            'features': dict(zip(self.feature_names, features))\n        }\n\n    def save(self, filepath='ml_classifier.pkl'):\n        if not self.is_trained:\n            raise ValueError(\"Model ch∆∞a ƒë∆∞·ª£c train! Kh√¥ng th·ªÉ l∆∞u.\")\n        with open(filepath, 'wb') as f:\n            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n        file_size = os.path.getsize(filepath) / 1024 / 1024\n        print(f\"‚úÖ ƒê√£ l∆∞u ML classifier v√†o: {filepath}\")\n        print(f\"   File size: {file_size:.2f} MB\")\n        print(f\"   Features: {len(self.feature_names)}\")\n\n    @staticmethod\n    def load(filepath='ml_classifier.pkl'):\n        if not os.path.exists(filepath):\n            raise FileNotFoundError(f\"File kh√¥ng t·ªìn t·∫°i: {filepath}\")\n        with open(filepath, 'rb') as f:\n            classifier = pickle.load(f)\n        print(f\"‚úÖ ƒê√£ load ML classifier t·ª´: {filepath}\")\n        print(f\"   Model type: {classifier.model_type}\")\n        print(f\"   Is trained: {classifier.is_trained}\")\n        print(f\"   Features: {len(classifier.feature_names)}\")\n        return classifier\n\nprint(\"=\"*60)\nprint(\"‚úÖ MLClassifier class created!\")\nprint(\"=\"*60)\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:53:16.080479Z","iopub.execute_input":"2025-11-23T01:53:16.080758Z","iopub.status.idle":"2025-11-23T01:53:16.100479Z","shell.execute_reply.started":"2025-11-23T01:53:16.080737Z","shell.execute_reply":"2025-11-23T01:53:16.099573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B∆Ø·ªöC 13B: TRAIN ML CLASSIFIER (WITH SAVE/LOAD)\n\nimport os\n\nML_CLASSIFIER_FILE = 'ml_classifier.pkl'\n\ntry:\n    active_detector = complete_detector_v2\n    print(\"‚úÖ S·ª≠ d·ª•ng CompletePlagiarismDetectorV2 (bi-encoder + doc scoring)\")\nexcept NameError:\n    raise ValueError(\"DETECTOR NOT FOUND!\\n  Variable 'complete_detector_v2' does not exist!\\n\")\n\nif os.path.exists(ML_CLASSIFIER_FILE):\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìÇ LOADING SAVED ML CLASSIFIER\")\n    print(\"=\"*60)\n    start_time = time.time()\n    ml_classifier = MLClassifier.load(ML_CLASSIFIER_FILE)\n    end_time = time.time()\n    print(f\"‚è±Ô∏è  Time: {end_time - start_time:.2f}s\")\nelse:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üî® TRAINING NEW ML CLASSIFIER\")\n    print(\"=\"*60)\n    ml_classifier = MLClassifier(model_type='xgboost')\n    try:\n        training_results = ml_classifier.train(\n            detector=active_detector,\n            queries_data=queries_data,\n            test_size=0.2,\n            verbose=True\n        )\n        print(f\"\\n{'='*60}\")\n        print(\"‚úÖ ML Classifier trained successfully!\")\n        print(\"=\"*60)\n        print(\"\\nüíæ Saving ML classifier...\")\n        ml_classifier.save(ML_CLASSIFIER_FILE)\n        print(\"üéâ Next run will load instantly!\")\n    except Exception as e:\n        print(f\"\\n‚ùå Error during training: {e}\")\n        import traceback\n        traceback.print_exc()\n        print(\"\\n‚ö†Ô∏è  SKIPPING ML CLASSIFIER TRAINING\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ML CLASSIFIER READY!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T01:53:20.885022Z","iopub.execute_input":"2025-11-23T01:53:20.885494Z","iopub.status.idle":"2025-11-23T02:10:51.09242Z","shell.execute_reply.started":"2025-11-23T01:53:20.88547Z","shell.execute_reply":"2025-11-23T02:10:51.091699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B∆Ø·ªöC 13C: ƒê√ÅNH GI√Å NHANH ML CLASSIFIER (KH√îNG D√ôNG THRESHOLD)\nprint(\"ML CLASSIFIER\")\nprint(\"=\"*60)\n\nif ml_classifier.is_trained:\n    test_indices = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n    comparison_results = []\n    \n    print(f\"\\n Testing tr√™n {len(test_indices)} queries...\")\n    print(\"-\"*60)\n    \n    for idx in test_indices:\n        query = queries_data[idx]\n        result = active_detector.detect(query['text'], verbose=False)\n        \n        ml_result = ml_classifier.predict(result)\n        ml_pred = ml_result['prediction']\n        ml_conf = ml_result['confidence']\n        true_label = query['is_plagiarism']\n        ml_correct = (ml_pred == true_label)\n        \n        comparison_results.append({\n            'query_id': query['id'],\n            'plag_type': query['plagiarism_type'],\n            'true_label': true_label,\n            'ml_pred': ml_pred,\n            'ml_conf': ml_conf,\n            'ml_correct': ml_correct\n        })\n        \n        if idx in [0, 20, 40, 60, 80]:\n            print(f\"\\nQuery {query['id']} ({query['plagiarism_type']}):\")\n            print(f\"  True: {'PLAGIARISM' if true_label else 'ORIGINAL'}\")\n            print(f\"  ML Classifier: {'PLAGIARISM' if ml_pred else 'ORIGINAL'} (conf: {ml_conf:.3f}) {'‚úÖ' if ml_correct else '‚ùå'}\")\n    \n    ml_accuracy = sum(r['ml_correct'] for r in comparison_results) / len(comparison_results)\n    print(f\"\\n{'='*60}\")\n    print(\" ML CLASSIFIER SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Accuracy tr√™n t·∫≠p nh·ªè: {ml_accuracy*100:.2f}%\")\n    \n    wrong_cases = [r for r in comparison_results if not r['ml_correct']]\n    if wrong_cases:\n        print(f\"\\nC√°c tr∆∞·ªùng h·ª£p ƒëo√°n sai ({len(wrong_cases)} cases):\")\n        for case in wrong_cases:\n            print(f\"  Query {case['query_id']} ({case['plag_type']}): true={'PLAG' if case['true_label'] else 'ORG'}, pred={'PLAG' if case['ml_pred'] else 'ORG'}, conf={case['ml_conf']:.3f}\")\n    else:\n        print(\"\\nML classifier ƒëo√°n ƒë√∫ng to√†n b·ªô c√°c m·∫´u th·ª≠ n√†y!\")\nelse:\n    print(\" ML Classifier ch∆∞a ƒë∆∞·ª£c train!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T02:11:10.759186Z","iopub.execute_input":"2025-11-23T02:11:10.759755Z","iopub.status.idle":"2025-11-23T02:11:11.9412Z","shell.execute_reply.started":"2025-11-23T02:11:10.759732Z","shell.execute_reply":"2025-11-23T02:11:11.940444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B∆Ø·ªöC 13D: FULL EVALUATION V·ªöI ML CLASSIFIER\nprint(\" FULL EVALUATION: ML CLASSIFIER\")\nprint(\"=\"*60)\n\nif ml_classifier.is_trained:\n    print(f\" Evaluating tr√™n to√†n b·ªô {len(queries_data)} queries...\")\n    print(\"(Qu√° tr√¨nh n√†y m·∫•t ~30-60 gi√¢y...)\")\n    \n    ml_predictions = []\n    true_labels_full = []\n    ml_confidences = []\n    \n    start_time = time.time()\n    \n    for i, query in enumerate(queries_data):\n        if (i + 1) % 20 == 0:\n            print(f\"   Progress: {i+1}/{len(queries_data)}\")\n        \n        result = active_detector.detect(query['text'], verbose=False)\n        ml_result = ml_classifier.predict(result)\n        \n        ml_predictions.append(int(ml_result['prediction']))\n        true_labels_full.append(int(query['is_plagiarism']))\n        ml_confidences.append(float(ml_result['confidence']))\n    \n    end_time = time.time()\n    \n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n    \n    ml_acc = accuracy_score(true_labels_full, ml_predictions)\n    ml_prec = precision_score(true_labels_full, ml_predictions)\n    ml_rec = recall_score(true_labels_full, ml_predictions)\n    ml_f1 = f1_score(true_labels_full, ml_predictions)\n    cm_ml = confusion_matrix(true_labels_full, ml_predictions)\n    \n    print(f\"\\n Evaluation completed! Time: {end_time - start_time:.2f}s\")\n    print(f\"\\n{'='*60}\")\n    print(\" ML CLASSIFIER PERFORMANCE\")\n    print(\"=\"*60)\n    print(f\"Accuracy : {ml_acc*100:.2f}%\")\n    print(f\"Precision: {ml_prec*100:.2f}%\")\n    print(f\"Recall   : {ml_rec*100:.2f}%\")\n    print(f\"F1-Score : {ml_f1*100:.2f}%\")\n    \n    print(f\"\\nConfusion Matrix:\")\n    print(f\"                 Predicted\")\n    print(f\"              Original  Plagiarism\")\n    print(f\"Actual Original     {cm_ml[0][0]:3d}       {cm_ml[0][1]:3d}\")\n    print(f\"       Plagiarism   {cm_ml[1][0]:3d}       {cm_ml[1][1]:3d}\")\n    \n    ml_results_df = pd.DataFrame({\n        'query_id': [q['id'] for q in queries_data],\n        'true_label': true_labels_full,\n        'ml_pred': ml_predictions,\n        'ml_confidence': ml_confidences,\n        'plagiarism_type': [q['plagiarism_type'] for q in queries_data],\n        'ml_correct': [t == p for t, p in zip(true_labels_full, ml_predictions)]\n    })\n    \n    ml_results_df.to_csv('ml_results.csv', index=False, encoding='utf-8-sig')\n    print(f\"\\n ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o ml_results.csv (shape: {ml_results_df.shape})\")\n    \n    print(f\"\\n{'='*60}\")\n    print(\" PERFORMANCE BY PLAGIARISM TYPE\")\n    print(\"=\"*60)\n    print(f\"{'Type':<20} {'ML Accuracy':<15}\")\n    print(\"-\"*35)\n    for ptype in ml_results_df['plagiarism_type'].unique():\n        subset = ml_results_df[ml_results_df['plagiarism_type'] == ptype]\n        ml_acc_type = subset['ml_correct'].mean()\n        print(f\"{ptype:<20} {ml_acc_type*100:>6.2f}%\")\n    \n    print(f\"\\n{'='*60}\")\n    print(\"üéâ ML CLASSIFIER EVALUATION COMPLETED!\")\n    print(\"=\"*60)\nelse:\n    print(\" ML Classifier ch∆∞a ƒë∆∞·ª£c train!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T02:11:22.522979Z","iopub.execute_input":"2025-11-23T02:11:22.523712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B∆Ø·ªöC 13E: VISUALIZATION - ML CLASSIFIER (KH√îNG C·∫¶N THRESHOLD)\nprint(\" VISUALIZATION: ML CLASSIFIER\")\nprint(\"=\"*60)\n\nif ml_classifier.is_trained and 'ml_results_df' in locals():\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # 1. Metrics overview\n    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n    ml_scores = [ml_acc, ml_prec, ml_rec, ml_f1]\n    axes[0, 0].bar(metrics, ml_scores, color='coral', alpha=0.85)\n    axes[0, 0].set_ylabel('Score')\n    axes[0, 0].set_title('ML Classifier Performance Metrics')\n    axes[0, 0].set_ylim([0, 1.05])\n    axes[0, 0].grid(axis='y', alpha=0.3)\n    for i, score in enumerate(ml_scores):\n        axes[0, 0].text(i, score + 0.02, f'{score:.3f}', ha='center', fontsize=10)\n    \n    # 2. Confusion matrix heatmap\n    import seaborn as sns\n    sns.heatmap(cm_ml, annot=True, fmt='d', cmap='Reds',\n                xticklabels=['Original', 'Plagiarism'],\n                yticklabels=['Original', 'Plagiarism'],\n                cbar=False, ax=axes[0, 1], linewidths=1, linecolor='gray')\n    axes[0, 1].set_title(f'Confusion Matrix (Accuracy: {ml_acc*100:.2f}%)')\n    axes[0, 1].set_xlabel('Predicted')\n    axes[0, 1].set_ylabel('Actual')\n    \n    # 3. Accuracy by plagiarism type\n    plag_types = ml_results_df['plagiarism_type'].unique()\n    ml_accs_by_type = [ml_results_df[ml_results_df['plagiarism_type'] == p]['ml_correct'].mean()\n                       for p in plag_types]\n    axes[1, 0].bar(plag_types, ml_accs_by_type, color='skyblue', alpha=0.85)\n    axes[1, 0].set_ylabel('Accuracy')\n    axes[1, 0].set_title('Accuracy by Plagiarism Type')\n    axes[1, 0].set_ylim([0, 1.05])\n    axes[1, 0].tick_params(axis='x', rotation=45)\n    axes[1, 0].grid(axis='y', alpha=0.3)\n    for x, acc in zip(plag_types, ml_accs_by_type):\n        axes[1, 0].text(x, acc + 0.02, f'{acc:.2f}', ha='center', fontsize=9)\n    \n    # 4. Confidence distribution\n    axes[1, 1].hist(ml_results_df['ml_confidence'], bins=20, color='mediumseagreen', alpha=0.8)\n    axes[1, 1].set_title('ML Confidence Distribution')\n    axes[1, 1].set_xlabel('Confidence')\n    axes[1, 1].set_ylabel('Count')\n    axes[1, 1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('ml_classifier_visualization.png', dpi=300, bbox_inches='tight')\n    print(\" ƒê√£ l∆∞u visualization: ml_classifier_visualization.png\")\n    plt.show()\n    \n    print(f\"\\n{'='*60}\")\n    print(\" VISUALIZATION COMPLETED!\")\n    print(\"=\"*60)\nelse:\n    print(\" Ch∆∞a c√≥ k·∫øt qu·∫£ ML Classifier ƒë·ªÉ visualize! Ch·∫°y B∆∞·ªõc 13D tr∆∞·ªõc.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T02:31:45.174532Z","iopub.execute_input":"2025-11-23T02:31:45.175234Z","iopub.status.idle":"2025-11-23T02:31:47.322567Z","shell.execute_reply.started":"2025-11-23T02:31:45.175207Z","shell.execute_reply":"2025-11-23T02:31:47.321875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# B∆Ø·ªöC 13F: FEATURE IMPORTANCE VISUALIZATION\nprint(\"üìä FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\"*60)\n\nif ml_classifier.is_trained and hasattr(ml_classifier.model, 'feature_importances_'):\n    importances = ml_classifier.model.feature_importances_\n    feature_names = ml_classifier.feature_names\n    \n    # Sort by importance\n    indices = np.argsort(importances)[::-1]\n    sorted_features = [feature_names[i] for i in indices]\n    sorted_importances = [importances[i] for i in indices]\n    \n    # Create figure\n    plt.figure(figsize=(10, 6))\n    bars = plt.barh(range(len(sorted_features)), sorted_importances, color='lightgreen', edgecolor='darkgreen')\n    plt.yticks(range(len(sorted_features)), sorted_features)\n    plt.xlabel('Importance Score')\n    plt.ylabel('Feature')\n    plt.title('Feature Importance in XGBoost Classifier', fontsize=14, fontweight='bold')\n    plt.grid(axis='x', alpha=0.3)\n    \n    # Add value labels\n    for i, (feature, imp) in enumerate(zip(sorted_features, sorted_importances)):\n        plt.text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=10)\n    \n    # Add color gradient based on importance\n    for i, bar in enumerate(bars):\n        bar.set_alpha(0.5 + 0.5 * (sorted_importances[i] / max(sorted_importances)))\n    \n    plt.tight_layout()\n    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n    print(\"‚úÖ ƒê√£ l∆∞u visualization: feature_importance.png\")\n    plt.show()\n    \n    # Print ranking\n    print(f\"\\nüìà FEATURE IMPORTANCE RANKING:\")\n    print(\"-\"*60)\n    for i, (feature, imp) in enumerate(zip(sorted_features, sorted_importances), 1):\n        print(f\"{i}. {feature:<20} {imp:.4f} {'‚ñà' * int(imp * 50)}\")\n    \n    print(f\"\\n{'='*60}\")\n    print(\"‚úÖ FEATURE IMPORTANCE ANALYSIS COMPLETED!\")\n    print(\"=\"*60)\n    \nelse:\n    print(\"‚ö†Ô∏è  ML Classifier ch∆∞a ƒë∆∞·ª£c train ho·∫∑c kh√¥ng c√≥ feature_importances_!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T02:31:56.703311Z","iopub.execute_input":"2025-11-23T02:31:56.703557Z","iopub.status.idle":"2025-11-23T02:31:57.410532Z","shell.execute_reply.started":"2025-11-23T02:31:56.70354Z","shell.execute_reply":"2025-11-23T02:31:57.409711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  DEMO: C√ÅCH S·ª¨ D·ª§NG DETECTOR + ML CLASSIFIER\n\nexample_query = \"\"\"T√¥i ƒëi h·ªçc\"\"\"\nprint(\"=\"*60)\nprint(\"üß™ DEMO: PLAGIARISM DETECTION\")\nprint(\"=\"*60)\nprint(f\"Query text:\\n{example_query}\\n\")\n\nresult = complete_detector_v2.detect(example_query, verbose=False)\n\nprint(f\"üìä DETECTOR RESULT:\")\nprint(f\"   Confidence: {result['confidence']:.3f}\")\nif result['best_match']:\n    print(f\"   Best match: {result['best_match']['doc_id']}\")\nelse:\n    print(\"   Best match: None\")\n\nif ml_classifier.is_trained:\n    ml_result = ml_classifier.predict(result)\n    print(f\"\\n ML CLASSIFIER RESULT:\")\n    print(f\"   Prediction: {'PLAGIARISM' if ml_result['prediction'] else 'ORIGINAL'}\")\n    print(f\"   Confidence: {ml_result['confidence']:.3f}\")\n    print(f\" FEATURE VALUES:\")\n    for feature, value in ml_result['features'].items():\n        print(f\"   {feature:<20} {value:.4f}\")\nelse:\n    print(\"\\n‚ö†Ô∏è  ML Classifier ch∆∞a ƒë∆∞·ª£c train!\")\n\nprint(f\"\\n{'='*60}\")\nprint(\" DEMO COMPLETED!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T02:32:04.05472Z","iopub.execute_input":"2025-11-23T02:32:04.055297Z","iopub.status.idle":"2025-11-23T02:32:04.12674Z","shell.execute_reply.started":"2025-11-23T02:32:04.055276Z","shell.execute_reply":"2025-11-23T02:32:04.125925Z"}},"outputs":[],"execution_count":null}]}